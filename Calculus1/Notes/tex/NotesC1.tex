% Ctrl + alt + b to build & preview (Linux)
\documentclass[11pt,a4paper]{article}

\usepackage[margin=1in, paperwidth=8.3in, paperheight=11.7in]{geometry}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{fancyhdr}

\begin{document}

\pagestyle{fancy}
\allowdisplaybreaks

\renewcommand{\headrulewidth}{0pt}
\newcommand{\vect}[1]{\boldsymbol{#1}}
\newcommand{\subtitle}[2]{\textbf{#1}\textit{#2} \\}
\newcommand{\dotprod}[0]{\boldsymbol{\cdot}}
\newcommand{\real}[0]{\mathbb{R}}
\newcommand{\nat}[0]{\mathbb{N}}
\setlength\parindent{0pt}

% Cover page title
\title{Calculus 1 - Notes}
\author{Dom Hutchinson}
\date{\today}
\maketitle

% Header
\fancyhead[L]{Dom Hutchinson}
\fancyhead[C]{Calculus 1 - Notes}
\fancyhead[R]{\today}

\tableofcontents

% Start of content
\newpage

\section{Before Calculus}

\subsection{Fundamental Theorem of Calculus}
\subtitle{Definition 1.01 - }{Fundamental Theorem of Calculus}
The Fundamental Theorem of Calculus states $$\frac{d}{dx}\int_{a}^{x} f(t) dt = f(x) $$ \\

\subtitle{Definition 1.02 - }{Common Sets of Numbers}
Natural Numbers, set of positive integers - $\nat := \{1, 2, 3, ...\}$. \\
Whole Numbers, set of all integers - $\mathbb{Z} := \{..., -2, -1, 0, 1, 2, ...\}$. \\
Rational Numbers, set of fractions - $\mathbb{Q} := \left\{\frac{p}{q} : p \in \mathbb{Z}, q \in \nat \right\}$. \\
Real Numbers, set of all rational \& irrational numbers - $\real$. \\

\subsection{Intervals}
\subtitle{Definition 1.03 - }{Intervals}
Sets of real numbers that fulfil in given ranges. \\
\underline{Notation}
\begin{eqnarray*}
  [a,b] := \{x \in \real : a \leq x \leq b\} \\
  (a,b] := \{x \in \real : a < x \leq b\} \\
  \left[a,b\right) := \{x \in \real : a \leq x < b\} \\
  (a,b) := \{x \in \real : a < x < b \}
\end{eqnarray*}

\underline{Example} \\
In what interval does x lie such that: $$|3x+4|<|2x-1|$$ \\
\textit{Solution}
\begin{align*}
  \mathrm{Case\ 1:}\ x \geq \frac{1}{2} & \\
  &=> 1 - 2x < 3x + 4 < 2x - 1 \\
  &=> 1 - 2x < 3x + 4 \\
  &=> x > \frac{-3}{5} \\ \\
  \mathrm{And,} &=> 3x + 4 < 2x - 1 \\
  &=> x < -5 \\
  \text{There are no real solutions in this range.} \\
  \mathrm{Case\ 2:}\ x < \frac{1}{2} & \\
  &=> 2x - 1 < 3x + 4 < 1 -2x \\
  &=> 2x - 1 < 3x + 4 \\
  &=> -5 < x \\
  \mathrm{And,} &=> 3x + 4 < 1 - 2x \\
  &=> 5x < -3 \\
  &=> x < \frac{-3}{5} \\
  %
  \\ &=>-5 < x < \frac{-3}{5},\ \underline{ x \in \left(-5,\frac{-3}{5}\right) }
\end{align*}

\subtitle{Definition 1.04 - }{Functions}
Functions map values between fields of numbers. The signature of a function is defined by $$f : A \to B$$
Where $f$ is the name of the function, $A$ is the domain and $B$ is the co-domain. \\
The \textit{Domain} of a function is the set of numbers it can take as an input. \\
The \textit{Co-Domain} is the set of numbers that the domain is mapped to. \\

\underline{N.B.} - A function is valid iff it maps each value in the domain to a single value in the co-domain. \\

\subtitle{Definition 1.05 - }{Maximal Domain}
The \textit{Maximal Domain} of a function is the largest set of values which can serve as the domain of a function. \\

\subtitle{Remark 1.06 - }{Types of Function}
Let $f:A \to B$ \\
\textit{Polynomials} $$f(x) = a_0 + a_1x + ... +a_nx^n$$
\textit{Rational} $$f(x) = \frac{p(x)}{q(x)},\quad q(x) \not = 0\ \forall\ x \in A$$
\textit{Trigonometric} $$sin(x),\ cos(x),\ tan(x)\ \mathrm{etc.}$$
%
\section{Limits}
\subsection{Limits}
\subtitle{Definition 2.01 - }{Limits}
A limit is the value a function tends to, for a given x. \\
\textit{i.e.} The value f(x) has at it gets very close to x. \\
\\\textit{Formally} We say $L$ is the limit of $f(x)$ as $x$ tends to $x_0$ if $$\forall\ \varepsilon > 0,\ \exists\ \delta > 0\ \mathrm{st\ if}\ x \in A\ \mathrm{and}\ |x - x_0| < \delta => |f(x) - L| < \varepsilon$$
\textit{Notation} $$\lim_{x \to x_0} f(x) = L$$

\newpage
\subtitle{Definition 2.02 - }{Directional Limits}
Sometimes the value of a limit depends on which direction you approach it from.\\
$\lim_{x \to x_{0}+}$ is used when approaching from values greater than $x_0$. \\
$\lim_{x \to x_{0}-}$ is used when approaching from values less than $x_0$. \\

\subtitle{Theorem 2.03 - }{Operations with limits}
Let $\lim_{x \to x_0} f(x) = L_f$ and $\lim_{x \to x_0} g(x) = L_g$
Then
\begin{alignat*}{2}
  &\lim_{x \to x_0} \left[f(x) + g(x)\right] &&= L_f + L_g \\
  &\lim_{x \to x_0} f(x).g(x) &&= L_f.L_g \\
  &\lim_{x \to x_0} \frac{f(x)}{g(x)} &&= \frac{L_f}{L_g} \quad L_g \not = 0
\end{alignat*}

\subsection{Exponential Function}
\subtitle{Definition 2.04 - }{Exponential Function}
$$e := \lim_{x \to \infty} \left(1+\frac{1}{n}\right)^n \simeq 2.7182818...$$\\

\subtitle{Theorem 2.05 - }{Binomial Expansion}
A techique for expanding binomial expressions
\begin{alignat*}{2}
\left(1+\frac{x}{n}\right)^n &= \sum_{i=0}^{n} \binom{i}{n} . 1^{(n-i)} . \left(\frac{x}{n}\right)^i \\
&= 1 + x + \frac{n-1}{2n}.x^2 + ... + \frac{x^n}{n^n}
\end{alignat*}

\section{The Derivative}

\subtitle{Definition 3.01 - }{Differentiable Equations}
Let $f : A \to B$ and $x_0 \in A$. \\
$f$ is differentiable at $x_0$ if $\exists\ L \in B$ such that $$L = \lim_{h \to 0} \frac{f(x_0 + h)-f(x_0)}{h}$$
If this limit exists $\forall\ x \in A$ then we can define the derivative of $f(x)$ $$f'(x) := \lim_{h \to 0} \frac{f(x + h) -f(x)}{h}$$

\subtitle{Definition 3.02 - }{Notation for Differentiation}
There are two ways to denote the derivative of an equation $$f'(x) \iff \frac{df}{dx}, f''(x) \iff \frac{d^2f}{dx^2}, ... , f^{(n)}(x) \iff \frac{d^nf}{dx^n}$$
\underline{N.B.} - Using $\displaystyle{\frac{df}{dx}}$ is more informative, especially for equations with multiple variables.

\subsection{Techniques for finding derivative}
%
\subtitle{Theorem 3.03 - }{Sum Rule}
Let $f, g$ be differentiable with respect to x.
$$(f+g)' = f' + g'$$

\subtitle{Theorem 3.04 - }{Product Rule}
Let $f, g$ be differentiable with respect to x.
$$(fg)' = f'g + fg'$$

\subtitle{Theorem 3.05 - }{Quotient Rule}
Let $f, g$ be differentiable with respect to x.
$$\left(\frac{f}{g}\right)' = \frac{f'g - fg'}{g^2}$$

\subtitle{Definition 3.06 - }{Composite Functions}
Let $f : B \to C$ and $g : A \to B$ Then $$(f \circ g)(x) = f(g(x))$$

\subtitle{Theorem 3.07 - }{Chain Rule}
Let $f, g$ be differentiable with respect to x.
$$\frac{d}{dx} f(g(x)) = f'(g(x)).g'(x)$$

\subsection{Implicit Differentiation}
%
\subtitle{Definition 3.08 - }{Implicit Differentiation}
Sometimes it is hard to isolate variables in multi-variable equations, in these cases differentiate both sides with respect to the same variable. \\
Remembering $$\frac{d}{dx}(x) = 1\ \mathrm{and}\ \frac{d}{dx}(y) = \frac{dy}{dx} = y'$$
\textit{Example} \\
Find $y$ if $x^3 + y^3 = 6xy$
\begin{alignat*}{2}
  &\frac{d}{dx}\left(x^3 + y^3\right) &&= \frac{d}{dx}\left(6xy\right) \\
  => &3x^2 + 3y^2.y' &&= 6y + 6x.y' \\
  => &y'(3y^2 - 6x) &&= 6y - 3x^2 \\
  => &y' &&= \frac{2y - x^2}{y^2 - 2x}
\end{alignat*}

\newpage
\subsection{Applications of The Derivative}
%
\subtitle{Thoerem 3.09 - }{Netwon's Method}
Let $f$ be differentiable. Using \textit{Newton's Method} we can approximate a solution to $f(x) = 0$.
\begin{enumerate}[label=\roman*)]
  \item Take an inital guess, $x_0$;
  \item Find the value of $x$ where the tangent to $x_0$ on $f(x)$ intercepts the x-axis;
  \item Use this value as the next guess;
  \item Repeat until the value of $x$ reduces little.
\end{enumerate}
The equation for the tangent is $$y = f(x_0) + (x - x_0)f'(x_0)$$ so a simplified equation for the process can be deduced $$x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}$$

\subtitle{Theorem 3.10 - }{Angle between Intersecting Curves}
Let $y = f_1(x)$ and $y = f_2(x)$ be two curves which intersect at $(x_0, y_0)$. \\
Then $y_0 = f_1(x_0) = f_2(x_0)$ \\
Let $m_1, m_2$ be the gradient of the tangents to $f_1$ \& $f_2$ at $x_0$. \\
Then $\theta_i := tan^{-1}(m_i)$ for $i = 1, 2$. \\
Let $\phi = |\theta_1 - \theta_2|$, then $$\phi = \left|\frac{m_1 - m_2}{1 + m_1m_2}\right|$$

\subtitle{Theorem 3.11 - }{L'Hospital's Rule}
For two equations equations, $f,g$ with limit of $-\infty, 0$ or $\infty$ as $x$ tends to $a$, it is hard to solve $$\lim_{x \to a}\frac{f(x)}{g(x)}$$
Provided the limit exists, L'Hospital's Rule states that $$\lim_{x \to a}\frac{f(x)}{g(x)} \iff \lim_{x \to a}\frac{f'(x)}{g'(x)}$$

\subsection{Sketching Curves}
%
\subtitle{Remark 3.11 - }{Sketching Curves}
Evaluating the derivative of a curve can make it easier to sketch:
\begin{enumerate}[label=\roman*)]
  \item When $f'(x) > 0$ the curve is heading upwards;
  \item When $f'(x) < 0$ the curve is heading downwards;
  \item When $f'(x) = 0$ the curve is flat;
  \item When $f'(x) = \infty, -\infty$ there are assymptotes.
\end{enumerate}

\subtitle{Definition 3.12 - }{Even Functions}
If $f(x) = f(-x)$ then the function is symmetrical and said to be \textit{even}. \\
\textit{Examples} - $x^2, cos(x), |x|$ \\

\subtitle{Definition 3.13 - }{Odd Functions}
If $f(x) = -f(-x)$ then the function is said to be \textit{odd}. \\
\textit{Examples} - $x, sin(x), x.cos(x)$\\

\subtitle{Remark 3.14}{}
Some functions are neither \textit{odd} nor \textit{even}. \\
\textit{Example} - $x + x^2$

\section{Inegration}
%
\subsection{The Primitive}

\subtitle{Definition 4.01 - }{The Primitive}
A function, $F : A \to \real$, is a primative for the function $f : A \to \real$ if $F$ is differentiable and $$\displaystyle{\frac{d}{dx} F = f}$$
\underline{N.B.} - Primitives are also called \textit{Indefinite Integral} or \textit{Anti-Derivative}. \\

\subtitle{Remark 4.02 - }{Area Under a Curve}
Let $f : [a,b] \to \real$ be a continuous function. Then the area between the curve and the x-axis is found by integration. $$A := \int_{a}^{b} f(x) dx$$

\subtitle{Definition 4.03 - }{Convergent Improper Integrals}
Let $b > a$ and define a function, $f : \left[a, \infty\right) \to \real$, which is continuous in $[a, b]$ Then
$$\int_{0}^{\infty} f(x) dx := \lim_{b \to \infty} \int_{a}^{b} f(x) dx$$
If this limit exists then the improper integral is \textit{convergent}, otherwise it is \textit{divergent}. \\

\subtitle{Definition 4.04 - }{Definite Integral}
Let $F$ be the primative for the function $f$. Then $$\int_{b}^{a} f(x) dx = F(a) - F(b)$$
\underline{Notation} - $F(x)\big|_{a}^{b} = F(b) - F(a)$\\

\subtitle{Remark 4.05 - }{Summing Definite Inegrals}
For all $a < c < b$ $$\int_{a}^{b} f(x) dx = \int_{c}^{a} f(x) dx + \int_{b}^{c} f(x) dx$$ $$\displaystyle{\int_{b}^{a} f(x) d := -\int_{a}^{b} f(x) dx}$$

\subtitle{Theorem 4.06 - }{Taylor Series}
Functions can be expanded into polynomial form with degree $n$, $T_n$, and remainder $R_n$ such that $f(x) = T_n(x) + R_n(x)$.
$$T_n(x) = f(a) + (x-a)f'(a) + ... + \frac{1}{n}.(x-a)^n.f^n(a)$$
$$R_n(x) = \frac{1}{n} \int_{a}^{x} (x-t)^n.f^{(n+1)}(t) dt$$

\section{Parametric Curves \& Arc-Length}
%
\subsection{Parametric Curves}
%
\subtitle{Definition 5.01 - }{Parametric Curves}
\textit{Parametric equations} are an alternative to Cartesian equations, for representing curves. They can also represent a point in 3D space.
$$\vect{p} = \begin{pmatrix} x(t)\\ y(t)\\ z(t) \end{pmatrix}$$

\subtitle{Theorem 5.02 - }{Parametric to Cartesian Equations}
As all equations in a Parametric series have a common variable, substition can be used to form a single equation. \\
\textit{Example} Let $\begin{pmatrix} x(t) \\ y(t) \end{pmatrix} = \begin{pmatrix} t-2 \\ \frac{t}{t-2} \end{pmatrix}$.
\begin{alignat*}{2}
  &x &&= t - 2 \\
  =>&t &&= x + 2 \\
  =>&y &&= \frac{x+2}{(x+2) - 2} \\
  & &&= \frac{x+2}{x} \\
  & y &&= \underline{1 + \frac{2}{x}}
\end{alignat*}

\subsection{Tangent of a Curve}
%
\subtitle{Theorem 5.02 - }{Tangent to a Parametric Curve}
Let $(x(t), y(t))$ be a parametric equation. If we want to find the tangent at a point on the line, $(a, b)$, we need to find the value $t_0$ such that $x(t_0) = a$ \& $y(t_0) = b$. \\
Then by using the chain rule we can deduce the following equation for the tangent when $t = t_0$: $$\frac{dy(t_0)}{dx(t_0)} = \frac{y - y(t_0)}{x - x(t_0)}$$.
Similarly we can deduce the equation for the normal when $t = t_0$: $$-\frac{dx(t_0)}{dy(t_0)} = \frac{y - y(t_0)}{x - x(t_0)}$$

\subsection{Arc-Length}
%
\subtitle{Theorem 5.03 - }{Arc-Length}
Arc-Length is the length of a curve, following a function, between two points. For a cartesian equation, $y = f(x)$, between the points $x$ and $x + dx$ is $$ds = \sqrt{dx^2 + dy^2}$$
So for a set of parametric equations, $(x(t), y(t)),\ a \leq t \leq b$, $$ds = \sqrt{\frac{dx^2}{dt} + \frac{dy^2}{dt}}$$
To find the length of a curve between points $a$ and $b$ $$s = \int_{a}^{b} \sqrt{\frac{dx^2}{dt} + \frac{dy^2}{dt}} dt$$

\subtitle{Definition 5.04 - }{Curvature}
Curvature measures how fast the unit tangent vector to a curve rotates.
Curvature of a curve, $y = f(x)$, can be found using the equation: $$K(x) = \frac{|y''(x)|}{[1 + (y'(x))^2]^{\frac{3}{2}}}$$
For a set of parametric equations, $(x(t), y(t))$, it can be found using: $$K(t_0) = \frac{y''(t_0).x'(t_0) - y'(t_0).x''(t_0)}{[(x'(t_0))^2 + (y'(t_0))^2]^{\frac{3}{2}}}$$

\subsection{Level Curves}
%
\subtitle{Definition 5.05 - }{Level Curves}
Let $f : \real^d \to \real$ be a function with $d \geq 2,\ d \in \nat$. A level curve for $f$ is the set of real solutions for $f(\vect{x}) = c, c \in \real$. \\
\underline{N.B} - $f(\vect{x}) = c$ is often written as $f = c$.

\section{Differential Equations}
%
\subtitle{Definition 6.01 - }{Differential Equations}
Differential equations take the form $$f(x, y, \frac{dx}{dy}, ... , \frac{d^{(n)}y}{dx^{(n)}}) = 0,\ x \in I$$

\subsection{First Order Differential Equations}
%
\subtitle{Definition 6.02 - }{First Order}
First order differential equations are equations of form $f(x, y, \frac{dx}{dy}) = 0$. \\

\subtitle{Definition 6.03 - }{Seperable Equations}
An equation, $f$, is said to be seperable if there exists two equations, $M(x),\ N(y)$, such that $$f(x, y, y') = y' - M(x).N(y)$$
Thus \begin{alignat*}{2}
  &y' &&= M(x).N(y) \\
  => &\frac{y'}{N(y)} &&= M(x) \\
  => &\int{\frac{1}{N(y)}dy} &&= \int{M(x)dx}
\end{alignat*}
After integration, the equation can be rearranged to be in terms of y.

\subsection{Integrating Factor}
%
\subtitle{Theorem 6.04 - }{Integrating Factor}
Consider the equation $y' + f(x)y + g(x)$. Let $F(x) = \int{f(x)dx}$. Thus \begin{alignat*}{2}
  &e^{F(x)}.y' + e^{F(x)}.y &&= e^{F(x)}.g(x) \\
  => &\frac{d}{dx}\left(e^{F(x)}.y \right) &&= e^{F(x)}.g(x) \\
  => &e^{F(x)}.y &&= \int{e^{F(x)}.g(x)\ dx} \\
  => &y &&= e^{-F(x)} \int{e^{F(x)}.g(x)\ dx}
\end{alignat*}

\subsection{Second Order Differential Equations}
%
\subtitle{Definition 6.05 - }{Linear Differential Equations}
A differential equation is said to be \textit{linear} if it can be written in the form $$Ay(x) := a_n(x).y^{(n)}(x) + ... + a_1(x).y'(x) + a_0(x).y(x) = b(x)$$
We define the set of solutions as $$S(A, b) := {y : I \to \real ; Ay = b}$$
If the only solution is $b=0$ then the system is homogenous, otherwise it is inhomogenous. \\

\subtitle{Definition 6.06 - }{Particular \& Complimentary Solutions}
When solving a differential equation, $Ay(x) = b(x)$, we need to find two functions in order to find the final solution.
\begin{enumerate}[label=\roman*)]
  \item Complementary Function, $y_c$ - The homogenous case of the equation, $Ay(x) = 0$;
  \item Particular Function, $y_p$ - The inhomogenous case of the equation, $Ay(x) = b(x)$ for a given $b(x)$.
\end{enumerate}
Then $y = y_c + y_p$ is the final solution for $Ay(x) = b(x)$.

\newpage
\subtitle{Theorem 6.07 - }{Complementary Functon of LDEs with Constant Coefficients}
Take a linear differential equation $$a_n.y^{(n)}(x) + ... + a_1.y'(x) + a_0.y(x) = b(x)$$ where $a_n, ... , a_1, a_0 \in \real\ \mathrm{\&}\ b(x) : \real \to \real$ are all constant. \\
To find the \textit{Complementary Function} we solve the equation $$a_n.\lambda^n + ... + a_1.\lambda^n + a_0 = 0$$.
to get solutions $\lambda_1, ... , \lambda_k$ and then produce the complimentary function $$y_c(x) = \mu_1e^{\lambda_1x} + ... + \mu_ke^{\lambda_kx}$$
Where $\mu_1, ... , \mu_k$ are constants to be found later, by comparing with $b(x)$.

\subtitle{Remark 6.08 - }{Complementary Function}
The complementary function, $y_c$, for differential equations with constant coefficients depends upon the $\lambda_1, ... , \lambda_k$ we find, due to Euler's Formula.
\begin{enumerate}[label=\roman*)]
  \item $\lambda_i = c,\quad y_{c_i} = \mu_ie^{\lambda_ix}$;
  \item $\lambda_i = \pm ib,\quad y_{c_i} = \mu_{i_1}cos(bx) + \mu_{i_2}sin(bx)$;
  \item $\lambda_i = a \pm ib,\quad y_{c_i} = e^{ax}[\mu_{i_1}cos(bx) + \mu_{i_2}sin(bx)]$.
\end{enumerate}
Then $y_c = \sum_{j=1}^{k} y_{c_j}$.\\

\subtitle{Remark 6.09 - }{Particular Function}
The particular function, $y_p$, for a differential equation with constand coefficients, $Ay(x) = b(x)$, depends on the form of $b(x)$.
\begin{enumerate}[label=\roman*)]
  \item $b(x) = a_nx^n + ... + a_1x + a_0,\quad y_p = b_nx^n + ... + b_1x + b_0$;
  \item $b(x) = ae^{bx},\quad y_p = \alpha e^{\beta x}$;
  \item $b(x) = a.sin(bx) + c.cos(dx),\quad y_p = \alpha sin(\beta x) + \gamma cos(\delta x)$.
\end{enumerate}
Where the constants of $y_p$ are values to be found, when given certain conditions.

\subtitle{Theorem 6.10 - }{Particular Functon of LDEs with Constant Coefficients}
Take a linear differential equation $$a_n.y^{(n)}(x) + ... + a_1.y'(x) + a_0.y(x) = b(x)$$ where $a_n, ... , a_1, a_0 \in \real\ \mathrm{\&}\ b(x) : \real \to \real$ are all constant. \\
Deduce the particular function for the differential equation, given $b(x)$, and then differentiate $y_p$ n times. \\
Substitute in these values, in place of the $y$s, into the original equation and solve to find values for the constants in $y_p$. \\

\underline{Example} \\
Solve $y'' -y' + y = x^2$.
\begin{align*}{2}
  \mathrm{\textbf{Complementary\ Function}} \\
  \mathrm{Let\ }\lambda^2 - \lambda + 1 &= 0 \\
  => \lambda &= \frac{1 \pm \sqrt{1-4}}{2} \\
  &= \frac{1}{2} \pm i\frac{\sqrt{3}}{2} \\
  => y_c &= e^{\frac{x}{2}}[A cos(x\frac{\sqrt{3}}{2}) + B sin(\frac{\sqrt{3}}{2})] \\
  \mathrm{\textbf{Particular\ Function}} \\
  \mathrm{Let\ }y_p &= \alpha x^2 + \beta x + \gamma \\
  => y'_p(x) &= 2\alpha x + \beta, \& \\
  => y''_p(x) &= 2\alpha \\
  => (2\alpha) - (2\alpha x + \beta) + (\alpha x^2 + \beta x + \gamma) &= x^2 \\
  => x^2[\alpha] + x[\beta - 2\alpha] + [2\alpha -\beta + \gamma] &= x^2 \\ \\
  [x^2] : \alpha &= 1 \\
  [x] : \beta - 2\alpha &= 0 \\
  =>\beta &= 2 \\
  [x^0] : 2\alpha + \gamma - \beta = 0 \\
  => \gamma &= 0 \\
  => y_p &= x^2 + 2x \\
  => \underline{y = x^2 + 2x + e^{\frac{x}{2}}[A cos(x\frac{\sqrt{3}}{2}) + B sin(\frac{\sqrt{3}}{2})]}
\end{align*}

\subsection{Wronskian}
%
\subtitle{Definition 6.11 - }{The Wronskian}
The \textit{Wronskian}, $W[y_1, y_2]$, of two differentiable functions is defined by $$W[y_1, y_2](x) = y_1(x).y'_2(x) - y'_1(x).y_2(x)$$
The notation for this is $$\begin{vmatrix} a & b \\ c & d \end{vmatrix} := ad - bc$$
So $$W[y_1, y_2] = \begin{vmatrix} y_1 & y_2 \\ y'_1 & y'_2 \end{vmatrix}$$

\subtitle{Remark 6.12}{}
If $W[y_1, y_2] \not = 0$ then $y_1, y_2$ are linearly independent.

\subsection{Variation of Constants}
%
\subtitle{Theorem 6.12}{}
This is a technique for solving all differential equations, not just ones with constant coefficients. \\
Consider the equation $$Ay(x) := y''(x) + \beta(x)y'(x) + \gamma(x)y(x) = b(x),\quad \mathrm{for\ a\ known\ }b(x)$$
We can deduce a complementary function in the form $$y_c = \lambda_1(x)y_1(x) + \lambda_2(x)y_2(x)$$ where $y_1\ \&\ y_2$ are linearly independent, thus $W[y_1, y_2] \not = 0$. \\
\huge{WIP}

\end{document}
