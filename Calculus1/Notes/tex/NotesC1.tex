% Ctrl + alt + b to build & preview (Linux)
\documentclass[11pt,a4paper]{article}

\usepackage[margin=1in, paperwidth=8.3in, paperheight=11.7in]{geometry}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{fancyhdr}

\begin{document}

\pagestyle{fancy}
\allowdisplaybreaks

\renewcommand{\headrulewidth}{0pt}
\newcommand{\vect}[1]{\boldsymbol{#1}}
\newcommand{\subtitle}[2]{\textbf{#1}\textit{#2} \\}
\newcommand{\dotprod}[0]{\boldsymbol{\cdot}}
\newcommand{\real}[0]{\mathbb{R}}
\newcommand{\nat}[0]{\mathbb{N}}
\setlength\parindent{0pt}

% Cover page title
\title{Calculus 1 - Notes}
\author{Dom Hutchinson}
\date{\today}
\maketitle

% Header
\fancyhead[L]{Dom Hutchinson}
\fancyhead[C]{Calculus 1 - Notes}
\fancyhead[R]{\today}

\tableofcontents

% Start of content
\newpage

\section{Before Calculus}

\subsection{Fundamental Theorem of Calculus}
\subtitle{Definition 1.01 - }{Fundamental Theorem of Calculus}
The Fundamental Theorem of Calculus states $$\frac{d}{dx}\int_{a}^{x} f(t) dt = f(x) $$ \\

\subtitle{Definition 1.02 - }{Common Sets of Numbers}
Natural Numbers, set of positive integers - $\nat := \{1, 2, 3, ...\}$. \\
Whole Numbers, set of all integers - $\mathbb{Z} := \{..., -2, -1, 0, 1, 2, ...\}$. \\
Rational Numbers, set of fractions - $\mathbb{Q} := \left\{\frac{p}{q} : p \in \mathbb{Z}, q \in \nat \right\}$. \\
Real Numbers, set of all rational \& irrational numbers - $\real$. \\

\subsection{Intervals}
\subtitle{Definition 1.03 - }{Intervals}
Sets of real numbers that fulfil in given ranges. \\
\underline{Notation}
\begin{eqnarray*}
  [a,b] := \{x \in \real : a \leq x \leq b\} \\
  (a,b] := \{x \in \real : a < x \leq b\} \\
  \left[a,b\right) := \{x \in \real : a \leq x < b\} \\
  (a,b) := \{x \in \real : a < x < b \}
\end{eqnarray*}

\underline{Example} \\
In what interval does x lie such that: $$|3x+4|<|2x-1|$$ \\
\textit{Solution}
\begin{align*}
  \mathrm{Case\ 1:}\ x \geq \frac{1}{2} & \\
  &=> 1 - 2x < 3x + 4 < 2x - 1 \\
  &=> 1 - 2x < 3x + 4 \\
  &=> x > \frac{-3}{5} \\ \\
  \mathrm{And,} &=> 3x + 4 < 2x - 1 \\
  &=> x < -5 \\
  \text{There are no real solutions in this range.} \\
  \mathrm{Case\ 2:}\ x < \frac{1}{2} & \\
  &=> 2x - 1 < 3x + 4 < 1 -2x \\
  &=> 2x - 1 < 3x + 4 \\
  &=> -5 < x \\
  \mathrm{And,} &=> 3x + 4 < 1 - 2x \\
  &=> 5x < -3 \\
  &=> x < \frac{-3}{5} \\
  %
  \\ &=>-5 < x < \frac{-3}{5},\ \underline{ x \in \left(-5,\frac{-3}{5}\right) }
\end{align*}

\subtitle{Definition 1.04 - }{Functions}
Functions map values between fields of numbers. The signature of a function is defined by $$f : A \to B$$
Where $f$ is the name of the function, $A$ is the domain and $B$ is the co-domain. \\
The \textit{Domain} of a function is the set of numbers it can take as an input. \\
The \textit{Co-Domain} is the set of numbers that the domain is mapped to. \\

\underline{N.B.} - A function is valid iff it maps each value in the domain to a single value in the co-domain. \\

\subtitle{Definition 1.05 - }{Maximal Domain}
The \textit{Maximal Domain} of a function is the largest set of values which can serve as the domain of a function. \\

\subtitle{Remark 1.06 - }{Types of Function}
Let $f:A \to B$ \\
\textit{Polynomials} $$f(x) = a_0 + a_1x + ... +a_nx^n$$
\textit{Rational} $$f(x) = \frac{p(x)}{q(x)},\quad q(x) \not = 0\ \forall\ x \in A$$
\textit{Trigonometric} $$sin(x),\ cos(x),\ tan(x)\ \mathrm{etc.}$$
%
\section{Limits}
\subsection{Limits}
\subtitle{Definition 2.01 - }{Limits}
A limit is the value a function tends to, for a given x. \\
\textit{i.e.} The value f(x) has at it gets very close to x. \\
\\\textit{Formally} We say $L$ is the limit of $f(x)$ as $x$ tends to $x_0$ if $$\forall\ \varepsilon > 0,\ \exists\ \delta > 0\ \mathrm{st\ if}\ x \in A\ \mathrm{and}\ |x - x_0| < \delta => |f(x) - L| < \varepsilon$$
\textit{Notation} $$\lim_{x \to x_0} f(x) = L$$

\newpage
\subtitle{Definition 2.02 - }{Directional Limits}
Sometimes the value of a limit depends on which direction you approach it from.\\
$\lim_{x \to x_{0}+}$ is used when approaching from values greater than $x_0$. \\
$\lim_{x \to x_{0}-}$ is used when approaching from values less than $x_0$. \\

\subtitle{Theorem 2.03 - }{Operations with limits}
Let $\lim_{x \to x_0} f(x) = L_f$ and $\lim_{x \to x_0} g(x) = L_g$
Then
\begin{alignat*}{2}
  &\lim_{x \to x_0} \left[f(x) + g(x)\right] &&= L_f + L_g \\
  &\lim_{x \to x_0} f(x).g(x) &&= L_f.L_g \\
  &\lim_{x \to x_0} \frac{f(x)}{g(x)} &&= \frac{L_f}{L_g} \quad L_g \not = 0
\end{alignat*}

\subsection{Exponential Function}
\subtitle{Definition 2.04 - }{Exponential Function}
$$e := \lim_{x \to \infty} \left(1+\frac{1}{n}\right)^n \simeq 2.7182818...$$\\

\subtitle{Theorem 2.05 - }{Binomial Expansion}
A techique for expanding binomial expressions
\begin{alignat*}{2}
\left(1+\frac{x}{n}\right)^n &= \sum_{i=0}^{n} \binom{i}{n} . 1^{(n-i)} . \left(\frac{x}{n}\right)^i \\
&= 1 + x + \frac{n-1}{2n}.x^2 + ... + \frac{x^n}{n^n}
\end{alignat*}

\section{The Derivative}

\subtitle{Definition 3.01 - }{Differentiable Equations}
Let $f : A \to B$ and $x_0 \in A$. \\
$f$ is differentiable at $x_0$ if $\exists\ L \in B$ such that $$L = \lim_{h \to 0} \frac{f(x_0 + h)-f(x_0)}{h}$$
If this limit exists $\forall\ x \in A$ then we can define the derivative of $f(x)$ $$f'(x) := \lim_{h \to 0} \frac{f(x + h) -f(x)}{h}$$

\subtitle{Definition 3.02 - }{Notation for Differentiation}
There are two ways to denote the derivative of an equation $$f'(x) \iff \frac{df}{dx}, f''(x) \iff \frac{d^2f}{dx^2}, ... , f^{(n)}(x) \iff \frac{d^nf}{dx^n}$$
\underline{N.B.} - Using $\displaystyle{\frac{df}{dx}}$ is more informative, especially for equations with multiple variables.

\subsection{Techniques for finding derivative}
%
\subtitle{Theorem 3.03 - }{Sum Rule}
Let $f, g$ be differentiable with respect to x.
$$(f+g)' = f' + g'$$

\subtitle{Theorem 3.04 - }{Product Rule}
Let $f, g$ be differentiable with respect to x.
$$(fg)' = f'g + fg'$$

\subtitle{Theorem 3.05 - }{Quotient Rule}
Let $f, g$ be differentiable with respect to x.
$$\left(\frac{f}{g}\right)' = \frac{f'g - fg'}{g^2}$$

\subtitle{Definition 3.06 - }{Composite Functions}
Let $f : B \to C$ and $g : A \to B$ Then $$(f \circ g)(x) = f(g(x))$$

\subtitle{Theorem 3.07 - }{Chain Rule}
Let $f, g$ be differentiable with respect to x.
$$\frac{d}{dx} f(g(x)) = f'(g(x)).g'(x)$$

\subsection{Implicit Differentiation}
%
\subtitle{Definition 3.08 - }{Implicit Differentiation}
Sometimes it is hard to isolate variables in multi-variable equations, in these cases differentiate both sides with respect to the same variable. \\
Remembering $$\frac{d}{dx}(x) = 1\ \mathrm{and}\ \frac{d}{dx}(y) = \frac{dy}{dx} = y'$$
\textit{Example} \\
Find $y$ if $x^3 + y^3 = 6xy$
\begin{alignat*}{2}
  &\frac{d}{dx}\left(x^3 + y^3\right) &&= \frac{d}{dx}\left(6xy\right) \\
  => &3x^2 + 3y^2.y' &&= 6y + 6x.y' \\
  => &y'(3y^2 - 6x) &&= 6y - 3x^2 \\
  => &y' &&= \frac{2y - x^2}{y^2 - 2x}
\end{alignat*}

\newpage
\subsection{Applications of The Derivative}
%
\subtitle{Thoerem 3.09 - }{Netwon's Method}
Let $f$ be differentiable. Using \textit{Newton's Method} we can approximate a solution to $f(x) = 0$.
\begin{enumerate}[label=\roman*)]
  \item Take an inital guess, $x_0$;
  \item Find the value of $x$ where the tangent to $x_0$ on $f(x)$ intercepts the x-axis;
  \item Use this value as the next guess;
  \item Repeat until the value of $x$ reduces little.
\end{enumerate}
The equation for the tangent is $$y = f(x_0) + (x - x_0)f'(x_0)$$ so a simplified equation for the process can be deduced $$x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}$$

\subtitle{Theorem 3.10 - }{Angle between Intersecting Curves}
Let $y = f_1(x)$ and $y = f_2(x)$ be two curves which intersect at $(x_0, y_0)$. \\
Then $y_0 = f_1(x_0) = f_2(x_0)$ \\
Let $m_1, m_2$ be the gradient of the tangents to $f_1$ \& $f_2$ at $x_0$. \\
Then $\theta_i := tan^{-1}(m_i)$ for $i = 1, 2$. \\
Let $\phi = |\theta_1 - \theta_2|$, then $$\phi = \left|\frac{m_1 - m_2}{1 + m_1m_2}\right|$$

\subtitle{Theorem 3.11 - }{L'Hospital's Rule}
For two equations equations, $f,g$ with limit of $-\infty, 0$ or $\infty$ as $x$ tends to $a$, it is hard to solve $$\lim_{x \to a}\frac{f(x)}{g(x)}$$
Provided the limit exists, L'Hospital's Rule states that $$\lim_{x \to a}\frac{f(x)}{g(x)} \iff \lim_{x \to a}\frac{f'(x)}{g'(x)}$$

\subsection{Sketching Curves}
%
\subtitle{Remark 3.11 - }{Sketching Curves}
Evaluating the derivative of a curve can make it easier to sketch:
\begin{enumerate}[label=\roman*)]
  \item When $f'(x) > 0$ the curve is heading upwards;
  \item When $f'(x) < 0$ the curve is heading downwards;
  \item When $f'(x) = 0$ the curve is flat;
  \item When $f'(x) = \infty, -\infty$ there are assymptotes.
\end{enumerate}

\subtitle{Definition 3.12 - }{Even Functions}
If $f(x) = f(-x)$ then the function is symmetrical and said to be \textit{even}. \\
\textit{Examples} - $x^2, cos(x), |x|$ \\

\subtitle{Definition 3.13 - }{Odd Functions}
If $f(x) = -f(-x)$ then the function is said to be \textit{odd}. \\
\textit{Examples} - $x, sin(x), x.cos(x)$\\

\subtitle{Remark 3.14}{}
Some functions are neither \textit{odd} nor \textit{even}. \\
\textit{Example} - $x + x^2$

\section{Inegration}
%
\subsection{The Primitive}

\subtitle{Definition 4.01 - }{The Primitive}
A function, $F : A \to \real$, is a primative for the function $f : A \to \real$ if $F$ is differentiable and $$\displaystyle{\frac{d}{dx} F = f}$$
\underline{N.B.} - Primitives are also called \textit{Indefinite Integral} or \textit{Anti-Derivative}. \\

\subtitle{Remark 4.02 - }{Area Under a Curve}
Let $f : [a,b] \to \real$ be a continuous function. Then the area between the curve and the x-axis is found by integration. $$A := \int_{a}^{b} f(x) dx$$

\subtitle{Definition 4.03 - }{Convergent Improper Integrals}
Let $b > a$ and define a function, $f : \left[a, \infty\right) \to \real$, which is continuous in $[a, b]$ Then
$$\int_{0}^{\infty} f(x) dx := \lim_{b \to \infty} \int_{a}^{b} f(x) dx$$
If this limit exists then the improper integral is \textit{convergent}, otherwise it is \textit{divergent}. \\

\subtitle{Definition 4.04 - }{Definite Integral}
Let $F$ be the primative for the function $f$. Then $$\int_{b}^{a} f(x) dx = F(a) - F(b)$$
\underline{Notation} - $F(x)\big|_{a}^{b} = F(b) - F(a)$\\

\subtitle{Remark 4.05 - }{Summing Definite Inegrals}
For all $a < c < b$ $$\int_{a}^{b} f(x) dx = \int_{c}^{a} f(x) dx + \int_{b}^{c} f(x) dx$$ $$\displaystyle{\int_{b}^{a} f(x) d := -\int_{a}^{b} f(x) dx}$$

\subtitle{Theorem 4.06 - }{Taylor Series}
Functions can be expanded into polynomial form with degree $n$, $T_n$, and remainder $R_n$ such that $f(x) = T_n(x) + R_n(x)$.
$$T_n(x) = f(a) + (x-a)f'(a) + ... + \frac{1}{n}.(x-a)^n.f^n(a)$$
$$R_n(x) = \frac{1}{n} \int_{a}^{x} (x-t)^n.f^{(n+1)}(t) dt$$

\section{Parametric Curves \& Arc-Length}
%
\subsection{Parametric Curves}
%
\subtitle{Definition 5.01 - }{Parametric Curves}
\textit{Parametric equations} are an alternative to Cartesian equations, for representing curves. They can also represent a point in 3D space.
$$\vect{p} = \begin{pmatrix} x(t)\\ y(t)\\ z(t) \end{pmatrix}$$

\subtitle{Theorem 5.02 - }{Parametric to Cartesian Equations}
As all equations in a Parametric series have a common variable, substition can be used to form a single equation. \\
\textit{Example} Let $\begin{pmatrix} x(t) \\ y(t) \end{pmatrix} = \begin{pmatrix} t-2 \\ \frac{t}{t-2} \end{pmatrix}$.
\begin{alignat*}{2}
  &x &&= t - 2 \\
  =>&t &&= x + 2 \\
  =>&y &&= \frac{x+2}{(x+2) - 2} \\
  & &&= \frac{x+2}{x} \\
  & y &&= \underline{1 + \frac{2}{x}}
\end{alignat*}

\subsection{Tangent of a Curve}
%
\subtitle{Theorem 5.02 - }{Tangent to a Parametric Curve}
Let $(x(t), y(t))$ be a parametric equation. If we want to find the tangent at a point on the line, $(a, b)$, we need to find the value $t_0$ such that $x(t_0) = a$ \& $y(t_0) = b$. \\
Then by using the chain rule we can deduce the following equation for the tangent when $t = t_0$: $$\frac{dy(t_0)}{dx(t_0)} = \frac{y - y(t_0)}{x - x(t_0)}$$.
Similarly we can deduce the equation for the normal when $t = t_0$: $$-\frac{dx(t_0)}{dy(t_0)} = \frac{y - y(t_0)}{x - x(t_0)}$$

\subsection{Arc-Length}
%
\subtitle{Theorem 5.03 - }{Arc-Length}
Arc-Length is the length of a curve, following a function, between two points. For a cartesian equation, $y = f(x)$, between the points $x$ and $x + dx$ is $$ds = \sqrt{dx^2 + dy^2}$$
So for a set of parametric equations, $(x(t), y(t)),\ a \leq t \leq b$, $$ds = \sqrt{\frac{dx^2}{dt} + \frac{dy^2}{dt}}$$
To find the length of a curve between points $a$ and $b$ $$s = \int_{a}^{b} \sqrt{\frac{dx^2}{dt} + \frac{dy^2}{dt}} dt$$

\subtitle{Definition 5.04 - }{Curvature}
Curvature measures how fast the unit tangent vector to a curve rotates.
Curvature of a curve, $y = f(x)$, can be found using the equation: $$K(x) = \frac{|y''(x)|}{[1 + (y'(x))^2]^{\frac{3}{2}}}$$
For a set of parametric equations, $(x(t), y(t))$, it can be found using: $$K(t_0) = \frac{y''(t_0).x'(t_0) - y'(t_0).x''(t_0)}{[(x'(t_0))^2 + (y'(t_0))^2]^{\frac{3}{2}}}$$

\subsection{Level Curves}
%
\subtitle{Definition 5.05 - }{Level Curves}
Let $f : \real^d \to \real$ be a function with $d \geq 2,\ d \in \nat$. A level curve for $f$ is the set of real solutions for $f(\vect{x}) = c, c \in \real$. \\
\underline{N.B} - $f(\vect{x}) = c$ is often written as $f = c$.

\section{Differential Equations}
%
\subtitle{Definition 6.01 - }{Differential Equations}
Differential equations take the form $$f(x, y, \frac{dx}{dy}, ... , \frac{d^{(n)}y}{dx^{(n)}}) = 0,\ x \in I$$

\subsection{First Order Differential Equations}
%
\subtitle{Definition 6.02 - }{First Order}
First order differential equations are equations of form $f(x, y, \frac{dx}{dy}) = 0$. \\

\subtitle{Definition 6.03 - }{Seperable Equations}
An equation, $f$, is said to be seperable if there exists two equations, $M(x),\ N(y)$, such that $$f(x, y, y') = y' - M(x).N(y)$$
Thus \begin{alignat*}{2}
  &y' &&= M(x).N(y) \\
  => &\frac{y'}{N(y)} &&= M(x) \\
  => &\int{\frac{1}{N(y)}dy} &&= \int{M(x)dx}
\end{alignat*}
After integration, the equation can be rearranged to be in terms of y.

\subsection{Integrating Factor}
%
\subtitle{Theorem 6.04 - }{Integrating Factor}
Consider the equation $y' + f(x)y + g(x)$. Let $F(x) = \int{f(x)dx}$. Thus \begin{alignat*}{2}
  &e^{F(x)}.y' + e^{F(x)}.y &&= e^{F(x)}.g(x) \\
  => &\frac{d}{dx}\left(e^{F(x)}.y \right) &&= e^{F(x)}.g(x) \\
  => &e^{F(x)}.y &&= \int{e^{F(x)}.g(x)\ dx} \\
  => &y &&= e^{-F(x)} \int{e^{F(x)}.g(x)\ dx}
\end{alignat*}

\subsection{Second Order Differential Equations}
%
\subtitle{Definition 6.05 - }{Linear Differential Equations}
A differential equation is said to be \textit{linear} if it can be written in the form $$Ay(x) := a_n(x).y^{(n)}(x) + ... + a_1(x).y'(x) + a_0(x).y(x) = b(x)$$
We define the set of solutions as $$S(A, b) := {y : I \to \real ; Ay = b}$$
If the only solution is $b=0$ then the system is homogenous, otherwise it is inhomogenous. \\

\subtitle{Definition 6.06 - }{Particular \& Complimentary Solutions}
When solving a differential equation, $Ay(x) = b(x)$, we need to find two functions in order to find the final solution.
\begin{enumerate}[label=\roman*)]
  \item Complementary Function, $y_c$ - The homogenous case of the equation, $Ay(x) = 0$;
  \item Particular Function, $y_p$ - The inhomogenous case of the equation, $Ay(x) = b(x)$ for a given $b(x)$.
\end{enumerate}
Then $y = y_c + y_p$ is the final solution for $Ay(x) = b(x)$.

\newpage
\subtitle{Theorem 6.07 - }{Complementary Functon of LDEs with Constant Coefficients}
Take a linear differential equation $$a_n.y^{(n)}(x) + ... + a_1.y'(x) + a_0.y(x) = b(x)$$ where $a_n, ... , a_1, a_0 \in \real\ \mathrm{\&}\ b(x) : \real \to \real$ are all constant. \\
To find the \textit{Complementary Function} we solve the equation $$a_n.\lambda^n + ... + a_1.\lambda^n + a_0 = 0$$.
to get solutions $\lambda_1, ... , \lambda_k$ and then produce the complimentary function $$y_c(x) = \mu_1e^{\lambda_1x} + ... + \mu_ke^{\lambda_kx}$$
Where $\mu_1, ... , \mu_k$ are constants to be found later, by comparing with $b(x)$.

\subtitle{Remark 6.08 - }{Complementary Function}
The complementary function, $y_c$, for differential equations with constant coefficients depends upon the $\lambda_1, ... , \lambda_k$ we find, due to Euler's Formula.
\begin{enumerate}[label=\roman*)]
  \item $\lambda_i = c,\quad y_{c_i} = \mu_ie^{\lambda_ix}$;
  \item $\lambda_i = \pm ib,\quad y_{c_i} = \mu_{i_1}cos(bx) + \mu_{i_2}sin(bx)$;
  \item $\lambda_i = a \pm ib,\quad y_{c_i} = e^{ax}[\mu_{i_1}cos(bx) + \mu_{i_2}sin(bx)]$.
\end{enumerate}
Then $y_c = \sum_{j=1}^{k} y_{c_j}$.\\

\subtitle{Remark 6.09 - }{Particular Function}
The particular function, $y_p$, for a differential equation with constand coefficients, $Ay(x) = b(x)$, depends on the form of $b(x)$.
\begin{enumerate}[label=\roman*)]
  \item $b(x) = a_nx^n + ... + a_1x + a_0,\quad y_p = b_nx^n + ... + b_1x + b_0$;
  \item $b(x) = ae^{bx},\quad y_p = \alpha e^{\beta x}$;
  \item $b(x) = a.sin(bx) + c.cos(dx),\quad y_p = \alpha sin(\beta x) + \gamma cos(\delta x)$.
\end{enumerate}
Where the constants of $y_p$ are values to be found, when given certain conditions.

\subtitle{Theorem 6.10 - }{Particular Functon of LDEs with Constant Coefficients}
Take a linear differential equation $$a_n.y^{(n)}(x) + ... + a_1.y'(x) + a_0.y(x) = b(x)$$ where $a_n, ... , a_1, a_0 \in \real\ \mathrm{\&}\ b(x) : \real \to \real$ are all constant. \\
Deduce the particular function for the differential equation, given $b(x)$, and then differentiate $y_p$ n times. \\
Substitute in these values, in place of the $y$s, into the original equation and solve to find values for the constants in $y_p$. \\

\underline{Example} \\
Solve $y'' -y' + y = x^2$.
\begin{align*}{2}
  \mathrm{\textbf{Complementary\ Function}} \\
  \mathrm{Let\ }\lambda^2 - \lambda + 1 &= 0 \\
  => \lambda &= \frac{1 \pm \sqrt{1-4}}{2} \\
  &= \frac{1}{2} \pm i\frac{\sqrt{3}}{2} \\
  => y_c &= e^{\frac{x}{2}}[A cos(x\frac{\sqrt{3}}{2}) + B sin(\frac{\sqrt{3}}{2})] \\
  \mathrm{\textbf{Particular\ Function}} \\
  \mathrm{Let\ }y_p &= \alpha x^2 + \beta x + \gamma \\
  => y'_p(x) &= 2\alpha x + \beta, \& \\
  => y''_p(x) &= 2\alpha \\
  => (2\alpha) - (2\alpha x + \beta) + (\alpha x^2 + \beta x + \gamma) &= x^2 \\
  => x^2[\alpha] + x[\beta - 2\alpha] + [2\alpha -\beta + \gamma] &= x^2 \\ \\
  [x^2] : \alpha &= 1 \\
  [x] : \beta - 2\alpha &= 0 \\
  =>\beta &= 2 \\
  [x^0] : 2\alpha + \gamma - \beta = 0 \\
  => \gamma &= 0 \\
  => y_p &= x^2 + 2x \\
  => \underline{y = x^2 + 2x + e^{\frac{x}{2}}[A cos(x\frac{\sqrt{3}}{2}) + B sin(\frac{\sqrt{3}}{2})]}
\end{align*}

\subsection{Wronskian}
%
\subtitle{Definition 6.11 - }{The Wronskian}
The \textit{Wronskian}, $W[y_1, y_2]$, of two differentiable functions is defined by $$W[y_1, y_2](x) = y_1(x).y'_2(x) - y'_1(x).y_2(x)$$
The notation for this is $$\begin{vmatrix} a & b \\ c & d \end{vmatrix} := ad - bc$$
So $$W[y_1, y_2] = \begin{vmatrix} y_1 & y_2 \\ y'_1 & y'_2 \end{vmatrix}$$

\subtitle{Remark 6.12}{}
If $W[y_1, y_2] \not = 0$ then $y_1, y_2$ are linearly independent.

\subsection{Variation of Constants}
%
\subtitle{Theorem 6.13}{}
This is a technique for solving all differential equations, not just ones with constant coefficients, and assumes we know the complementary function, $y_c$. \\
Consider the equation $$Ay(x) := y''(x) + \beta(x)y'(x) + \gamma(x)y(x) = b(x),\quad \mathrm{for\ a\ known\ }b(x)$$
Suppose we have a complementary function in the form $$y_c = \lambda_1(x)y_1(x) + \lambda_2(x)y_2(x)$$ where $y_1\ \&\ y_2$ are linearly independent, thus $W[y_1, y_2] \not = 0$. \\
Then $$y'_p = \lambda'_1 y_1 + \lambda_1 y'_1 + \lambda'_2 y_2 + \lambda_2 y'_2$$
As $\lambda_1, \lambda_2$ are constant then $\lambda'_1 = \lambda'_2 = 0$ so $$y'_p = \lambda_1y'_1 + \lambda_2y'_2$$
By differentiating and then substituting back into the original equation we see $y_p$ is a solution iff $$\lambda'_1y'_1 + \lambda'_2y'_2 = f$$
In matrix form we have $$\begin{pmatrix} y_1 & y_2 \\ y'_1 & y'_2 \end{pmatrix} \begin{pmatrix} \lambda'_1 \\ \lambda'_2 \end{pmatrix} = \begin{pmatrix} 0 \\ f \end{pmatrix}$$
Then by Cramer's rule we have $$\lambda'_1 = \frac{\begin{vmatrix}0 & y_2 \\ f & y'_2 \end{vmatrix}}{W[y_1\ y_2]} = \frac{-y_2f}{W[y_1\ y_2]}$$
and $$\lambda'_2 = \frac{\begin{vmatrix}y_1 & 0 \\ y'_1 & f \end{vmatrix}}{W[y_1\ y_2]} = \frac{y_1f}{W[y_1\ y_2]}$$
Giving use a solution for $y'_p = \lambda'_1 y_1 + \lambda_1 y'_1 + \lambda'_2 y_2 + \lambda_2 y'_2$.

\section{Applied Differential Equations}
%
\subtitle{Definition 7.01 - }{Denoting Limit Relationships}
We use $$F(x) \sim G(x)\ \mathrm{as}\ x \to a$$ to denote $$\lim_{x \to a} \frac{F(x)}{G(x)} = 1$$

\subtitle{Theorem 7.02 - }{Vibrating String}
If we are given a string which is $L$ long then we can define an equation, $y(x,t)$, which describe the displacement of a point $x$ along the string, at time $t$. $$y(x,t) = u(x)e^{i\omega t}$$
Where $\frac{\omega}{2\pi}$ is the frequency of the string and $u(x) = Acos(\omega x) + Bsin(\omega x)$. \\
We can generalise this for strings with $n$ anti-nodes. $$\omega_n := \frac{n\pi}{L},\quad u_n := sin(\omega_nx)$$

\section{Liner Difference Equations}
%
\subtitle{Definition 8.01 - }{Difference Equations}
A difference equation is an equation of the form $$F(n, y_n, ... , y_{n+d}) = 0,\quad n,d \in \nat$$

\subsection{First-Order Linear Difference Equation}
\subtitle{Definition 8.02 - }{Liner First-Order Difference Equations}
A \textit{Linear First-Order Difference Equation} is an equation, $F$, which can be described by $$F(n, y_n, y_{n+1}) = a_ny_{n+1} + b_ny_n - f_n$$
where $a_n, b_n, f_n$ are all known sequences. \\

\subtitle{Example 8.03}{}
By taking a simple equation $$y_{n+1} - y_n = f_n$$
we can see that $$y_{n+1} = y_n +(y_{n+1} - y_n) = y_n + f_n = ... = y_{n_0} +f_{n_0} + ... + f_{n-1} + f_n$$
So $$y_n = y_{n_0} + \sum_{j = n_0}^{n-1} f_j$$

\subtitle{Theorem 8.04 - }{Solving First-Order Liner Difference Equations}
From \textit{Definition 8.02} we can generalise the equation to show that $$y_{n+1} + b_ny_n = f_n$$
Then $$\frac{-1}{b_n}y_{n+1}-y_n = \frac{-1}{b_n}f_n$$
We now define the \textit{Summing Factor}, $S_n$, as $$S_n :=\prod_{j={n_0}}^{n-1}\frac{-1}{b_n}$$.
We multiply both sides of the original equation by the summing factor and as $S_n(\frac{-1}{b_n})=S_{n+1}$ we get $$S_{n+1}y_{n+1} - S_ny_n = S_{n+1}f_n$$
As this has the same form as the example in \textit{8.03} we can now deduce $$S_ny_n = y_{n_0} + \sum_{j={n_0}}^{n-1}S_{j+1}.f_j$$

\subsection{Second-Order Linear Difference Equation}
%
\subtitle{Definition 8.05 - }{Second-Order Linear Difference Equation}
A \textit{Linear Second-Order Difference Equation} is an equation, $F$, which can be described by $$a_ny_{n+2} + b_ny_{n+1} +c_ny_n = f_n$$
where $a_n, b_n, c_n, f_n$ are know sequences. \\

\subtitle{Remark 8.06 - }{Solving Second-Order Linear Difference Equations}
Similar to solving second-order differential equations we need to consider to cases. The \textit{homogenous} \& \textit{inhomogenous} cases.
So two sequences will be found the complementary sequence, $y_n^c$, and the particular sequence, $y_n^p$. The final solution for $y_n$ is given by $$y_n = y_n^c + y_n^p$$

\subtitle{Definition 8.07 - }{Wronskian of Sequences}
For two sequences $u_n \& v_n$ we define the Wronskian to be $$W_n := \begin{vmatrix} u_n & v_n \\ u_{n+1} & v_{n+1} \end{vmatrix} = u_n.v_{n+1} - v_n.u_{n+1}$$

\subtitle{Theorem 8.08 - }{Homogenous Case with Constant Cofficients}
Take the equation $$ay_{n+2} + by_{n+1} + cy_n = 0$$ where $a, b, c$ are known constants. We look for solutions of the form $$y_n = \lambda^n$$
By substition we get the equation $a\lambda^2 + b\lambda + c = 0$. By solving for $\lambda$ we find a solution
\begin{enumerate}[label=\roman*)]
  \item $\lambda$ has two real solutions - $y_n = A\lambda_1^n + B\lambda_2^n$;
  \item $\lambda$ has one real solution - $y_n = (An + B)\lambda^n$;
  \item $\lambda$ has only an imaginamry solution - $y_n = \Lambda e^{i\theta},\quad \Lambda^2 := \frac{c}{a}, \theta := tan^{-1}(\frac{4ac - b^2}{-b})$.\\
\end{enumerate}

\subtitle{Theorem 8.09 - }{Homogenous Case Second-Order Linear Difference Equations}
The homogenous case finds solutions for $$a_ny_{n+2} + b_ny_{n+1} + c_ny_n = 0$$
Suppose $y_n = u_n$ and $y_n = v_n$ are solutions to this homogenous equation. Then $$W_n [u_n\ v_n] = W_{n_0}\prod_{j=n_0}^{n-1} \frac{c_j}{a_j}$$
So $$ u_n.v_{n+1} - v_n.u_{n+1} = W_{n_0}\prod_{j=n_0}^{n-1} \frac{c_j}{a_j}$$
Which can be rearranged to be in the form of a first order difference equation, such as \textit{Example 8.03}
$$\frac{u_n}{u_{n+1}}v_{n+1} - v_n = W_{n_0}\prod_{j=n_0}^{n-1} \frac{c_j}{a_j}$$
Which has a summing factor $$S_n = \frac{1}{u_n}$$
By multiplying both sides by $S_n$ and simplifying we get
$$v_n = u_n \sum_{j=n_0}^{n-1} \frac{1}{u_j.u_{j+1}} \prod_{k=n_0}^{n-1} \frac{c_k}{a_k}$$
Typically you need to solve the product part of the equation to get a result for the sequence $u_n$. \\

\subtitle{Remark 8.10 - }{Inomogenous Case Second-Order Linear Difference Equations}
Generally the best way to do this is to make an educated guess based on the right hand side of the equation.
So if the RHS is a polynomial, guess a polynomial, etc. Similar to solving differential equations.

\section{Several Variables - Differentiability}
%
\subtitle{Definition 9.1 - }{Several Variable Function}
Let $d \in \nat, A \subset \real^d$ \& $B \subset \real^d$.\\
A function $\vect{f} : A \to B$ is a map which, for all $\vect{x} \in A$, assigns a unique value $\vect{f}(\vect{x}) \in B$.\\

\subtitle{Defintion 9.2 - }{Linear Functions}
A function $\vect{f} : \real^d \to \real^n$ is \textit{linear} if it can be given in terms of a matrix $A \in M_{n,d}(\real)$ where $$\vect{f}(\vect{x}) = A\vect{x}$$

\subtitle{Theorem 9.3 - }{Properties of Liner Functions}
If a function is linear then the following are true:
\begin{enumerate}[label=\roman*)]
  \item $\vect{f}(\lambda\vect{x}) = \lambda\vect{f}(\vect{x})$;
  \item $\vect{f}(\vect{x} + \vect{y}) = \vect{f}(\vect{x}) + \vect{f}(\vect{y})$.
\end{enumerate}

\subtitle{Definition 9.4 - }{Continuous Several Variable Function}
Let $\vect{f} : A \to B$ and $\vect{a} \in A$. Then $\vect{f}$ is continuous if $$\lim_{\vect{x} \to \vect{a}}\vect{f}(\vect{x}) = \vect{f}(\vect{a})$$

\subsection{The Derivative}
%
\subtitle{Defintion 9.5 - }{Norm of a Vector}
The norm of a vector, $\vect{x} \in \real^d$ is $$||\vect{x}|| := \left( \sum_{j=1}^{d}x_j \right)^{1/2}$$

\subtitle{Definition 9.6 - }{Derivative of Several Variable Function}
A function, $\vect{f} : \real^d \to \real^n$, is said to be differentiable at the point $\vect{x} \in \real^d$ if the exists an $A \in M_{n,d}(\real)$ such that
$$\lim_{\vect{h} \to \vect{0}} \frac{||\vect{f}(\vect{x} + \vect{h}) - \vect{f}(\vect{x}) -A\vect{h}||}{||\vect{h}||} = 0$$
This $A$ is the derivative of $\vect{f}(\vect{x})$. $$\vect{f}'(\vect{x}) := A$$

\subtitle{Remark 9.7}{}
If consider the following several variable function
$$\vect{f} = \begin{pmatrix} f_1 \\ \vdots \\ f_n \end{pmatrix}$$
Then
$$\vect{f}' = \begin{pmatrix} f'_1 \\ \vdots \\ f'_n \end{pmatrix}$$

\section{Directional \& Partial Derivatives}
%
\subsection{Directional Derivative}

\subtitle{Definition 10.1 - }{Direction}
A direction in $\real^d$ is a vector of unit length. \\
In $\real^2$ every direction can be given by $\vect{u} = \begin{pmatrix}cos(\theta)\\ sin(\theta)\end{pmatrix}$, where $\theta$ is the angle from positive $x$ axis \\
In $\real^3$ every direction can be given by $\vect{u} = \begin{pmatrix}sin(\phi)cos(\theta)\\ sin(\phi)sin(\theta)\\ cos(\phi)\end{pmatrix}$, where $\phi$ is the angle from positive $z$ axis and $\theta$ is the angle from the positive $x$ axis.

\subtitle{Definition 10.2 - }{Spherical Co-ordinates}
The spherical co-ordinates describe points in three dimension space.\\
The distance of a point from the origin is $$r = \rho sin(\phi)$$
Then $$\vect{x} = \begin{pmatrix} x \\ y \\ z \end{pmatrix} = \begin{pmatrix} r cos(\theta) \\ r sin(\theta) \\ \rho cos(\phi) \end{pmatrix} = \begin{pmatrix} \rho sin(\phi)cos(\theta) \\ \rho sin(\phi)sin(\theta) \\ \rho cos(\phi) \end{pmatrix}$$

\subtitle{Defintion 10.3 - }{Direction Derivative}
The direction derivative of $\vect{f}$ in the direction of $\vect{u}$ at the point $\vect{x}_0$ is the vector:
$$D_{\vect{u}}\vect{f}(\vect{x}_0) := \frac{d}{dt} \vect{f}(\vect{x}_0 + t\vect{u})\bigm|_{t=0}$$

\subtitle{Theorem 10.4}{}
For all $\vect{f} : \real^d \to \real^n$ the directional derivative at $\vect{u}$ in $\real^d$ we have
$$D_{\vect{u}} \vect{f} (\vect{x}) = \vect{f}'(\vect{x}).\vect{u}$$

\subsection{Partial Derivative}

\subtitle{Defintion 10.5 - }{Partial Derivative}
Let $f : \real^d \to \real$. Then the direction derivative, $D_{\vect{e}_j}f(\vect{x})$, if it exists, is called the \textit{partial derivative} of $f$ with respect to $x_j$ at $\vect{x}$.\\
This is denoted by $$\frac{\partial f}{\partial x_j}(\vect{x}) \mathrm{\ or\ } f_{x_j}(\vect{x})$$

\subtitle{Proposition 10.6 - }{Partial Derivative as a Matrix}
If $\vect{f} : \real^d \to \real^n$ is differentiable then
$$\vect{f}'(\vect{x}) = \begin{pmatrix}
\frac{\partial f_1}{\partial x_1} & \dots & \frac{\partial f_1}{\partial x_d} \\
\vdots & \ddots & \vdots \\
\frac{\partial f_n}{\partial x_1} & \dots & \frac{\partial f_n}{\partial x_d}
\end{pmatrix}$$

\subtitle{Remark 10.7 - }{Second Order Partial Derivatives}
\begin{alignat*}{2}
  &\frac{\partial^2 f}{\partial x^2} &&= \frac{\partial}{\partial x} \dotprod \frac{\partial f}{\partial x} \\
  &\frac{\partial^2 f}{\partial x \partial y} &&= \frac{\partial}{\partial x} \dotprod \frac{\partial f}{\partial y} \\
  &\frac{\partial^2 f}{\partial y^2} &&= \frac{\partial}{\partial y} \dotprod \frac{\partial f}{\partial y}
\end{alignat*}

\section{Gradient \& Chain Rule in Several Variables}
%
\subsection{Chain Rule}

\subtitle{Theorem 11.1 - }{Chain rule in Several Variables}
Let $\vect{f} : \real^d \to \real^n$ \& $\vect{g} : \real^m \to \real^d$ be differentiable. Then
$$[\vect{f} \dotprod \vect{g}]' = \vect{f}'(\vect{g}(\vect{x})).\vect{g}'(\vect{x})$$

\subsection{Invertible Maps \& Implicit Differentiation}

\subtitle{Definition 11.2 - }{Implicit Differentiation}
Let $A, B \subset \real^d$ and $\vect{f} : A \to B$.\\
If $\vect{f}$ is invertible then be cannot denote the inverse by $\vect{f}^{-1} : B \to A$ and $$[\vect{f}^{-1} \dotprod \vect{f}](\vect{x}) = \vect{x}$$
If $\vect{f}$ and $\vect{f}^{-1}$ are differentiable then by the chain run $$\vect{f}' (\vect{f}^{-1} (\vect{x})) \vect{f}^{-1}\ '(\vect{x}) = I$$
Hence $$\vect{f}^{-1}\ '(\vect{x}) = [\vect{f}'(\vect{f}(\vect{x}))]^{-1}$$

\subsection{The Gradient}

\subtitle{Defintion 11.3 - }{The Gradient}
Let $f : \real^d \to \real$ be a map with first-order partial derivatives at every point of $\real^d$.\\
Then the gradient at a point $\vect{x} \in \real^d$, $\nabla f : \real^d \to \real^d$, is defined as $$\nabla f(\vect{x}) := (f_{x_1}(\vect{x}), \dots, f_{x_d}(\vect{x}))$$

\subtitle{Definition 11.4 - }{Direction of Greatest Change}
By considering the case in two dimensions then $$(D_{\vect{u}}f)(x,y) = \nabla f(x,y) \dotprod \vect{u}$$
Where $\vect{u} = (cos\theta, sin\theta)$. \\
Then $$(D_{\vect{u}}f)(x_0,y_0) = cos\theta.\frac{\partial f}{\partial x}(x_0, y_0) + sin\theta.\frac{\partial f}{\partial y}(x_0, y_0)$$
The greatest change occurs when
\begin{alignat*}{2}
  &\frac{d}{d\theta} [cos\theta.\frac{\partial f}{\partial x}(x_0, y_0) + sin\theta.\frac{\partial f}{\partial y}(x_0, y_0)] &&= 0 \\
  =>&-sin\theta.\frac{\partial f}{\partial x}(x_0, y_0) + cos\theta.\frac{\partial f}{\partial y}(x_0, y_0)] &&= \\
  =>&\nabla f(x_0, y_0) \dotprod (-sin\theta, cos\theta) &&= \\
\end{alignat*}
We can thus deduce that $$\nabla f(x_0, y_0) \perp (-sin\theta, cos\theta)$$
So $$\vect{u} \parallel \nabla f(x_0, y_0)$$
We can then establish the direction, $\vect{u}$ occurs when $\vect{u}$ satisfies $$\vect{u} = \frac{\nabla f(x_0, y_0)}{||\nabla f(x_0, y_0)||}$$

\section{Integration over Two-Dimensional Domains}

\subtitle{Remark 12.1 - }{Notation}
Let $D \subset \real^2$.\\
When integrating over $D$ the following notation is used $$\iint_D f(x, y) dx dy$$
When $D$ is specified e.g. $D = \{(x, y) : g(y) < x < h(y); a < y < b\}$ then we denote it by $$\int_{a}^{b} \left\{\int_{g(y)}^{h(y)} f(x, y) dx \right\} dy$$

\subtitle{Theorem 12.2 - }{Rectangular Domain}
If we are given a domain between constant values, $D = \{(x, y) : c < y < d; a < x < b\}$ then we perform
$$V = \int_{c}^{d} \left\{\int_{a}^{b} f(x, y) dx \right\} dy$$
And can perform the integrations are seperate calculations.

\subtitle{Theorem 12.3 - }{Triangle Domain}
In triangular domains the value of $x$ depends on $y$, so can be expressed as a function of $y$. If we take $0 < x < a$ and $0 < y < b$ then $x = \frac{ya}{b}$.
This gives us a domain of $$D = \{(x, y): \frac{ya}{b} < x < a; 0 < y < b\}$$
We can solve the intregral by performing: $$V = \int_{0}^{b} \left\{ \int_{\frac{ay}{b}}^{a} f(x, y) dx \right\} dy$$

\subtitle{Theorem 12.4 - }{Genreal Domains}
If we have a domain, $D$, define by $D = \{(x, y) \in \real^2: L(x) \leq y \leq U(x), a \leq x \leq b\}$. Then the integral over $D$ can be found by
$$V=\int_{a}^{b} \left\{ \int_{L(x)}^{U(x)} f(x,y) dy \right\} dx$$

\end{document}
