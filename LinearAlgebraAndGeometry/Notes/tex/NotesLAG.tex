% Ctrl + alt + b to build & preview (Linux)
\documentclass[11pt,a4paper]{article}

\usepackage[margin=1in, paperwidth=8.3in, paperheight=11.7in]{geometry}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{fancyhdr}

\begin{document}

\pagestyle{fancy}
\setlength\parindent{0pt}
\allowdisplaybreaks

\renewcommand{\headrulewidth}{0pt}
\newcommand{\vect}[1]{\boldsymbol{#1}}
\newcommand{\subtitle}[2]{\textbf{#1}\textit{#2} \\}
\newcommand{\dotprod}[0]{\boldsymbol{\cdot}}

% Cover page title
\title{Linear Algebra \& Geometry - Notes}
\author{Dom Hutchinson}
\date{\today}
\maketitle

% Header
\fancyhead[L]{Dom Hutchinson}
\fancyhead[C]{Linear Algebra \& Geometry - Notes}
\fancyhead[R]{\today}

\tableofcontents

% Start of content
\newpage

\section{Euclidean Plane, Vectors, Cartesian Co-Ordinates \& Complex Numbers}

\subsection{Vectors}

\subtitle{Definition 1.01 - }{Vectors}
Ordered sets of real numbers. \\
Denoted by $\vect{v} = (v_1, v_2, v_3,...) = \begin{pmatrix} x \\ y \end{pmatrix}$ \\

\subtitle{Definition 1.02 - }{Euclidean Plane}
The set of two dimensional vectors, with real componenets, is called the Euclidean Plane. \\
Denoted by $\mathbb{R}^2$ \\

\subtitle{Definition 1.03 - }{Vector Addition}
Let $\vect{v}, \vect{w} \in \mathbb{R}^2$ such that $\vect{v} = (v_1,v_2)$ and $\vect{w} = (w_1,w_2)$. \\
Then $\vect{v} + \vect{w} = (v_1 + w_1, v_2 + w_2)$. \\

\subtitle{Definition 1.03 - }{Scalar Multiplcation of Vectors}
Let $\vect{v} \in \mathbb{R}^2$ and $\lambda \in \mathbb{R}$ such that $\vect{v} = (v_1,v_2)$. \\
Then $\lambda\vect{v} = (\lambda v_1, \lambda v_2)$. \\

\subtitle{Definition 1.04 - }{Norm of vectors}
The norm of a vector is its length from the origin. \\
Denoted by $||\vect{v}|| = \sqrt{v_{1}^{2} + v_{2}^{2}}$ for $\vect{v} \in \mathbb{R}^2$. \\

\subtitle{Theorem 1.05}{}
Let $\vect{v}, \vect{w} \in \mathbb{R}^2$ and $\lambda \in \mathbb{R} $ such that $\vect{v} = (v_1,v_2)$ and $\vect{w} = (w_1,w_2)$. \\
Then
\begin{align*}
  ||\vect{v}|| &= 0\ \mathrm{iff}\ \vect{v} = \vect{0}\\
  ||\lambda\vect{v}|| &= \sqrt{\lambda^2v_{1}^{2} + \lambda^2v_{2}^{2}} \\
  &= |\lambda|.||\vect{v}|| \\
  ||\vect{v} + \vect{w}|| &\leq ||\vect{v}|| + ||\vect{w}||
\end{align*} \\

\subtitle{Definition 1.06 - }{Unit Vector}
A vector can be described by its length \& direction. \\
Let $\vect{v} \in \mathbb{R}^2\backslash\{\vect{0}\}$. \\
Then $\vect{v} = ||\vect{v}||\vect{u}$ where $\vect{u}$ is the unit vector, $\vect{u} = \begin{pmatrix} cos\theta \\ sin\theta \end{pmatrix}$ \\
Thus $\forall\ \vect{v} \in \mathbb{R}^2\ \vect{v} = \begin{pmatrix} \lambda cos\theta \\ \lambda sin\theta \end{pmatrix}$ for some $\lambda \in \mathbb{R}$. \\

\subtitle{Definition 1.07 - }{Dot Product}
Let $\vect{v} \in \mathbb{R}^2$ and $\lambda \in \mathbb{R}$ such that $\vect{v} = (v_1,v_2)$. \\
Then $\vect{v} \dotprod \vect{w} = v_1.w_1 + v_2.w_2$. \\

\subtitle{Remark 1.08 - }{Positivity of Dot Product}
Let $\vect{v} \in \mathbb{R}^2$. \\
Then $\vect{v} \dotprod \vect{v} = ||\vect{v}||^2 = v_{1}^{2} + v_{2}^{2} \geq 0$. \\

\subtitle{Remark 1.09 - }{Angle between vectors in Euclidean Plane}
Let $\vect{v}, \vect{w} \in \mathbb{R}^2$. \\
Set $\theta$ to be the angle between $\vect{v}$ \& $\vect{w}$. \\
Then $$cos\theta = \frac{\vect{v} \dotprod \vect{w}}{||\vect{v}||\ ||\vect{w}||}$$.

\subtitle{Theorem 1.10 - }{Cauchy-Schwarz Inequality}
Let $\vect{v}, \vect{w} \in \mathbb{R}^2$. \\
Then $$|\vect{v} \dotprod \vect{w}| \leq ||\vect{v}||\ ||\vect{w}||$$
%
\textit{Proof}
\begin{align*}
  \frac{v_1w_1}{||\vect{v}||\ ||\vect{w}||} + \frac{v_2w_2}{||\vect{v}||\ ||\vect{w}||} &\leq \frac{1}{2}\left(\frac{v_{1}^{2}}{||\vect{v}||^2} + \frac{w_{1}^{2}}{||\vect{w}||^2}\right) + \frac{1}{2}\left(\frac{v_{2}^{2}}{||\vect{v}||^2} + \frac{w_{2}^{2}}{||\vect{w}||^2}\right) \\
  &\leq \frac{1}{2}\left(\frac{v_{1}^{2} + v_{2}^{2}}{||\vect{v}||^2} + \frac{w_{1}^{2} + w_{2}^{2}}{||\vect{w}||^2}\right) \\
  &\leq \frac{1}{2}(1 + 1) \\
  &\leq 1 \\
  => |v_1w_1 + v_2w_2| &\leq ||\vect{v}||\ ||\vect{w}|| \\
  |\vect{v} \dotprod \vect{w}| &\leq ||\vect{v}||\ ||\vect{w}|| \\
\end{align*}

\subsection{Complex Numbers}

\subtitle{Definition 1.11 - }{i}
\begin{alignat*}{1}
  i^2 &= -1 \\
  i &= \sqrt{-1}
\end{alignat*}

\subtitle{Definition 1.12 - }{Complex Number Set}
The set of complex numbers contains all numbers with an imaginary part. $$\mathbb{C} := \left\{x + iy; x,y \in \mathbb{R}\right\}$$
Complex numbers are often denoted by $$z = x + iy$$ and we say $x$ is the real part of $z$ and $y$ the imaginary part.

\subtitle{Definition 1.13 - }{Complex Conjugate}
Let $z \in \mathbb{C}$ st $z = x + iy$. \\
Then $$\bar{z} := x - iy$$
%
\newpage
%
\subtitle{Theorem 1.14 - }{Operations on Complex Numbers}
Let $z_1,z_2 \in \mathbb{C}$ st $z_1 = x_1 + iy_1$ and $z_2 = x_2 + iy_2$. \\
Then \begin{alignat*}{2}
  &z_1 + z_2 &&:= (x_1 + x_2) + i(y_1 + y_2) \\
  &z_1.z_2 &&:= (x_1 + iy_1)(x_2 + iy_2) \\
  & && := x_1.x_2 - y_1.y_2 + i(x_1.y_2 + x_2.y_1)
\end{alignat*}
\underline{N.B.} When dividing by a complex number, multiply top and bottom by the complex conjugate. \\

\subtitle{Definition 1.15 - }{Modulus of Complex Numbers}
The modulus of a complex number is the distance of the number, from the origin, on an Argand diagram.
Let $z \in \mathbb{C}$ st $z = x + iy$. \\
Then \begin{alignat*}{2}
  |z| &:= \sqrt{x^2 + y^2} \\
  &:= \sqrt{\bar{z}z}
\end{alignat*}
\underline{N.B.} Amplitude is an alternative name for the modulus \\

\subtitle{Definition 1.16 - }{Phase of Complex Numbers}
The phase of a complex number is the angle between the positive real axis and the line subtended from the origin and the number, on an Argand digram.$$z = |z|.(cos\theta + i.sin\theta), \quad \theta = \mathrm{Phase}$$
\underline{N.B.} Phase of $\bar{z}$ = - Phase of $z$ \\

\subtitle{Theorem 1.17 - }{de Moivre's Formula}
$$z^n = (cos\theta +i.sin\theta)^n = cos(n\theta)+i.sin(n\theta)$$

\subtitle{Theorem 1.18 - }{Euler's Formula}
$$e^{i\theta} = cos\theta + i.sin\theta$$

\subtitle{Remark 1.19}{}
Using Euler's formula we can express all complex numbers in terms of $e$. Thus many properties of the exponential remain true:
\begin{alignat*}{2}
  z &= \lambda e^{i\theta}, && \quad \lambda \in \mathbb{R} , \theta \in \left[0, 2\pi\right) \\
  => z_1 + z_2 &= \lambda_1 . \lambda_2. e^{i(\theta_1 + \theta_2)} \\
  \&, \frac{z_1}{z_2} &= \frac{\lambda_1}{\lambda_2}.e^{i(\theta_1 = \theta_2)}
\end{alignat*}

\newpage
\section{Euclidean Space, $\mathbb{R}^n$}
%
\subtitle{Definition 2.01 - }{Euclidean Space}
Let $n \in \mathbb{N}$ then $\forall\ \vect{x} = (x_1, x_2, ... , x_n)$ with $x_1, x_2, ... , x_n \in \mathbb{R}$ we have that $\vect{x} \in \mathbb{R}^n$. \\

\subtitle{Theorem 2.02 - }{Operations in Euclidean Space}
Let $\vect(x), \vect(y) \in \mathbb{R}^n$ and $\lambda \in \mathbb{R}$.
Then $$\vect(x) + \vect(y) = (x_1 + y_1, ... , x_n + y_n)$$
And $$\vect(x) + \lambda.\vect(y) = (x_1 + \lambda.y_1, ... , x_n + \lambda.y_n)$$

\subtitle{Definition 2.03 - }{Cartesian Product}
Let $A, B \in \mathbb{R}^n$ be non-empty sets. \\
Then $$A \times B := \{(a,b); a \in A, b \in B\}$$

\subsection{Dot Product}
%
\subtitle{Definition 2.04 - }{Dot Product}
Let $\vect{v}, \vect{w} \in \mathbb{R}^n$. Then \begin{alignat*}{1}
\vect{v} \dotprod \vect{w} &:= v_1.w_1 + ... + v_n.w_n \\
&:= \sum_{j=1}^n v_j.w_j
\end{alignat*}

\subtitle{Theorem 2.05 - }{Properties of the Dot Product}
Let $\vect{u}, \vect{v}, \vect{w} \in \mathbb{R}^n$.
Linearity: $$(\vect{u} + \lambda\vect{v}) \dotprod \vect{w} = \vect{u} \dotprod \vect{w} + \lambda(\vect{v} \dotprod \vect{w})$$
Symmetry: $$\vect{v} \dotprod \vect{w} = \vect{w} \dotprod \vect{v}$$
Positivity: $$\vect{v} \dotprod \vect{v} = v_1^2 + v_2^2 + ... +v_n^2 \geq 0$$

\subtitle{Definition 2.06 - }{Orthogonality}
Let $\vect{v}, \vect{w} \in \mathbb{R}^n$. \\
It is said that $\vect(v), \vect(w)$ are orthogonal to each other if $\vect{v} \dotprod \vect{w} = 0$ \\
\underline{N.B.} Orthogonal vectors are perpendicular to each other. \\

\subtitle{Definition 2.07 - }{The Norm}
Let $\vect{x} \in \mathbb{R}^n$.\\
Then $$||\vect{x}|| = \sqrt{\vect{x} \dotprod \vect{x}} = \sqrt{\sum_{i=1}^n x_i^2}$$

\newpage
%
\subtitle{Theorem 2.08 - }{Properties of the Norm}
Let $\vect{x}, \vect{y} \in \mathbb{R}^n$ and $\lambda \in \mathbb{R}$. Then
\begin{alignat*}{2}
  &||\vect{x}|| &&\geq 0 \\
  &||\vect{x}|| &&= 0 \mathrm{\ iff\ } \vect{x} = 0 \\
  &||\lambda\vect{x}|| &&= |\lambda| ||\vect{x}|| \\
  &||\vect{x} + \vect{y}|| &&\leq ||\vect{x}|| + ||\vect{y}||
\end{alignat*}

\subtitle{Theorem 2.09 - }{Dot Product and Norm}
Let $\vect{x}, \vect{y} \in \mathbb{R}^n$.
$$|\vect{x} \dotprod \vect{y}| \leq ||\vect{x}|| ||\vect{y}||$$
\underline{N.B.} $|\vect{x} \dotprod \vect{y}| = ||\vect{x}|| ||\vect{y}||$ iff $\vect{x}$ \& $\vect{x}$ are orthogonal.\\

\subtitle{Theorem 2.10 - }{Angle between Vectors}
Let $\vect{x}, \vect{y} \in \mathbb{R}^n$. Then $$cos\theta = \frac{\vect{x} \dotprod \vect{y}}{||\vect{x}|| ||\vect{y}||}$$

\subsection{Linear Subspaces}
%
\subtitle{Definition 2.11 - }{Linear Subspace}
Let $V \subset \mathbb{R}^n$. $V$ is a \textit{Linear Subspace} if: \\
\textbf{i)} $V \not = \emptyset$; \\
\textbf{ii)} $\forall\ \vect{v}, \vect{w} \in V$ then $\vect{v} + \vect{w} \in V$; \\
\textbf{iii)} $\forall\ \lambda \in \mathbb{R}, \vect{v} \in V$ then $\lambda\vect{v} \in V$. \\

\subtitle{Definition 2.12 - }{Span}
Let $\vect{x_1}, ... , \vect{x_k} \in \mathbb{R}^n; k \in \mathbb{N}$. Then
$$\mathrm{span}\{\vect{x_1}, ... , \vect{x_k}\} := \{\lambda_1\vect{x_1} + ... + \lambda_k\vect{x_k}; \lambda_i \in \mathbb{R}, 0 \leq i \geq k\}$$

\subtitle{Definition 2.13 - }{Spans are Subspaces}
Let $\vect{x_1}, ... , \vect{x_k} \in \mathbb{R}^n;\ k \in \mathbb{N}$. Then span$\{\vect{x_1}, ... , \vect{x_k}\}$ is a linear subspace of $\mathbb{R}^n$. \\

\subtitle{Theorem 2.14}{}
$$W_{\vect{a}} := \{\vect{x} \in \mathbb{R}^n; \vect{x} \dotprod \vect{a} = 0\}\mathrm{\ is\ a\ subspace.}$$

\subtitle{Definition 2.15 - }{Orthogonal Complement}
Let $V \subset \mathbb{R}^n$. Then,
$$V^{\perp} := \{\vect{x} \in \mathbb{R}^n; \vect{x} \dotprod \vect{y}\ \forall\ \vect{y} \in V\}$$
\underline{N.B.} $V^{\perp} \subset \mathbb{R}^n$

\subtitle{Theorem 2.16 - }{Relationship of Subspaces}
Let $V, W$ be subspaces of $\mathbb{R}$. Then
$$V \cap W \mathrm{\ is\ a\ subspace.}$$
$$V+W := \{\vect{v} + \vect{w}; \vect{v} \in V, \vect{w} \in W\}\mathrm{\ is\ a\ subspace.}$$

\subtitle{Definition 2.17 - }{Direct Sum}
Let $V_1, V_2, W$ be subspaces of $\mathbb{R}$. Then $W$ is said to be a \textit{direct sum} if \\
\textbf{i)} $W = V_1 + V_2$; \\
\textbf{ii)} $V_1 \cap V_2 = \emptyset$. \\

\section{Linear Equations \& Matrices}

\subsection{Linear Equations}
%
\subtitle{Definition 3.01 - }{Multi-Variable Linear Equations}
Linear equations produce a straight line and can have multiple variables. \\
\textit{Examples} - $x = 3, y = x + 3, z + 5x - 2y$ \\

\subtitle{Defintion 3.02 - }{Systems of Linear Equations}
Let $\vect{a}, \vect{x} \in \mathbb{R}^n$ \& $b \in \mathbb{R}$ such that $\vect{a} \dotprod \vect{x} = b$. \\
$\vect{a} \dotprod \vect{x} = b$ is a linear equation in $x$ with $S = \{\vect{x}; \vect{a} \dotprod \vect{x} = b\}$ as the set of solutions. \\
\underline{N.B.} If $b = 0$ then $S(\vect{a}, 0)$ is a subspace. \\

\subtitle{Definition 3.03 - }{Matrix}
Let $m, n \in \mathbb{N}$, then a $m \times n$ grid of numbers form an "m" by "n" matrix.
Each element of the matrix can be reference by $a_{ij}$ with $i = 1, ... , m$ and $j = 1, ... , n$.
$$ A = \begin{pmatrix}
  a_{11} & a_{12} & ... & a_{1n} \\
  a_{21} & a_{22} & ... & a_{2n} \\
  \vdots & \vdots & \ddots & \vdots \\
  a_{m1} & a_{m2} & ... & a_{mn}
\end{pmatrix}
$$
\underline{N.B.} m,i = rows, n,j = columns \\

\subtitle{Definition 3.04 - }{Sets of Matrices}
$M_{m,n}(\mathbb{R})$ is the set of m x n matrices containing only real numbers. \\
$M_{m,n}(\mathbb{Z})$ is the set of m x n matrices containing only integers. \\
$M_{n}(\mathbb{R})$ is the set square matrices, size n, containing only real numbers. \\

\subtitle{Definition 3.05 - }{Transpose Vectors}
Let $\vect{x} = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n\end{pmatrix}$ then $\vect{x}^t = \begin{pmatrix} x_1 & x_2 & ... & x_n\end{pmatrix}$

\subtitle{Definition 3.06 - }{Vector-Matrix Multiplication}
Let $A \in \mathbb{R}_{m,n}$ and $\vect{x} \in \mathbb{R}^n$ then
$$A\vect{x} := \begin{pmatrix}
  a_{11} & a_{12} & ... & a_{1n} \\
  a_{21} & a_{22} & ... & a_{2n} \\
  \vdots & \vdots & \ddots & \vdots \\
  a_{m1} & a_{m2} & ... & a_{mn}
\end{pmatrix} \begin{pmatrix}
  x_1 \\ x_2 \\ \vdots \\ x_n
\end{pmatrix} = \begin{pmatrix}
  a_{11}x_1 + a_{12}x_2 + ... + a_{1n}{x_n} \\
  a_{21}x_1 + a_{22}x_2 + ... + a_{2n}{x_n} \\
  \vdots \\
  a_{m1}x_1 + a_{m2}x_2 + ... + a_{mn}{x_n} \\
\end{pmatrix} = \begin{pmatrix}
  \vect{a}_{1}^{t} \dotprod \vect{x} \\
  \vect{a}_{2}^{t} \dotprod \vect{x} \\
  \vdots \\
  \vect{a}_{m}^{t} \dotprod \vect{x} \\
\end{pmatrix} \in \mathbb{R}^m
$$ This can be simplified to $$\vect{y} = A\vect{x} \mathrm{\ with\ } y_i = \sum_{j=1}^n a_{ij} x_j$$

\subtitle{Theorem 3.07 - }{Operations on Matrices with Vectors}
\begin{enumerate}[label=\roman*)]
  \item $A(\vect{x} + \vect{y}) = A\vect{x} + A\vect{y},\quad \forall\ \vect{x}, \vect{y} \in \mathbb{R}^n$.
  \item $A(\lambda\vect{x}) = \lambda(A\vect{x}),\quad \forall \vect{x} \in \mathbb{R}^n, \lambda \in \mathbb{R}$.
\end{enumerate}

\subtitle{Theorem 3.08}{}
Let $A = (a_{ij}) \in M_{m,n}(\mathbb{R})$ and $B = (b_{ij}) \in M_{l,m}(\mathbb{R})$.
Then there exists a $C = (c_{ij}) \in M_{l,n}(\mathbb{R})$ such that
$$C\vect{x} = B(A\vect{x}), \quad \forall\ \vect{x} \in \mathbb{R}^n$$
\underline{N.B.} $c_{ij} = \sum_{k=1}^m b_{ik} a_{kj}$ \\

\subtitle{Theorem 3.09 - }{Operation between Matrices}
Let $A, B \in M_{m,n}$ and $C \in M_{l,m}$
\begin{enumerate}[label=\roman*)]
  \item $C(A + B) = CA + CB$.
  \item $(A + B)C = AC + BC$.
  \item Let $D \in M_{m,n}, E \in M_{n,l}$ \& $F \in M_{l, k}$ then $$E(FG) = (EF)G$$
\end{enumerate}
\underline{N.B.} $AB \not = BA$ \\

\subtitle{Definition 3.10 - }{Types of Matrix}
Upper Triangle $\begin{pmatrix}
1 & 2 & 3 \\
0 & 4 & 5 \\
0 & 0 & 6
\end{pmatrix},\quad a_{ij} = 0 \mathrm{\ if\ } i > j$. \\
Lower Triangle $\begin{pmatrix}
1 & 0 & 0 \\
2 & 3 & 0 \\
4 & 5 & 6
\end{pmatrix},\quad a_{ij} = 0 \mathrm{\ if\ } i < j$. \\
Symmetric Matrix $\begin{pmatrix}
1 & 2 & 3 \\
2 & 4 & 0 \\
3 & 0 & 1
\end{pmatrix},\quad a_{ij} = a_{ji}.$ \\
Anti-Symmetric $\begin{pmatrix}
1 & -2 & -3 \\
2 & 0 & -4 \\
3 & 4 & -1
\end{pmatrix},\quad a_{ij} = -a_{ji}$. \\

%\subtitle{Definition 3.11 - }{Transposed Matrices}

\end{document}
