% Ctrl + alt + b to build & preview (Linux)
\documentclass[11pt,a4paper]{article}

\usepackage[margin=1in, paperwidth=8.3in, paperheight=11.7in]{geometry}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{fancyhdr}

\begin{document}

\pagestyle{fancy}
\setlength\parindent{0pt}
\allowdisplaybreaks

\renewcommand{\headrulewidth}{0pt}
\newcommand{\vect}[1]{\boldsymbol{#1}}
\newcommand{\subtitle}[2]{\textbf{#1}\textit{#2} \\}
\newcommand{\dotprod}[0]{\boldsymbol{\cdot}}

% Cover page title
\title{Linear Algebra \& Geometry - Notes}
\author{Dom Hutchinson}
\date{\today}
\maketitle

% Header
\fancyhead[L]{Dom Hutchinson}
\fancyhead[C]{Linear Algebra \& Geometry - Notes}
\fancyhead[R]{\today}

\tableofcontents

% Start of content
\newpage

\section{Euclidean Plane, Vectors, Cartesian Co-Ordinates \& Complex Numbers}

\subsection{Vectors}

\subtitle{Definition 1.01 - }{Vectors}
Ordered sets of real numbers. \\
Denoted by $\vect{v} = (v_1, v_2, v_3,...) = \begin{pmatrix} x \\ y \end{pmatrix}$ \\

\subtitle{Definition 1.02 - }{Euclidean Plane}
The set of two dimensional vectors, with real componenets, is called the Euclidean Plane. \\
Denoted by $\mathbb{R}^2$ \\

\subtitle{Definition 1.03 - }{Vector Addition}
Let $\vect{v}, \vect{w} \in \mathbb{R}^2$ such that $\vect{v} = (v_1,v_2)$ and $\vect{w} = (w_1,w_2)$. \\
Then $\vect{v} + \vect{w} = (v_1 + w_1, v_2 + w_2)$. \\

\subtitle{Definition 1.03 - }{Scalar Multiplcation of Vectors}
Let $\vect{v} \in \mathbb{R}^2$ and $\lambda \in \mathbb{R}$ such that $\vect{v} = (v_1,v_2)$. \\
Then $\lambda\vect{v} = (\lambda v_1, \lambda v_2)$. \\

\subtitle{Definition 1.04 - }{Norm of vectors}
The norm of a vector is its length from the origin. \\
Denoted by $||\vect{v}|| = \sqrt{v_{1}^{2} + v_{2}^{2}}$ for $\vect{v} \in \mathbb{R}^2$. \\

\subtitle{Theorem 1.05}{}
Let $\vect{v}, \vect{w} \in \mathbb{R}^2$ and $\lambda \in \mathbb{R} $ such that $\vect{v} = (v_1,v_2)$ and $\vect{w} = (w_1,w_2)$. \\
Then
\begin{align*}
  ||\vect{v}|| &= 0\ \mathrm{iff}\ \vect{v} = \vect{0}\\
  ||\lambda\vect{v}|| &= \sqrt{\lambda^2v_{1}^{2} + \lambda^2v_{2}^{2}} \\
  &= |\lambda|.||\vect{v}|| \\
  ||\vect{v} + \vect{w}|| &\leq ||\vect{v}|| + ||\vect{w}||
\end{align*} \\

\subtitle{Definition 1.06 - }{Unit Vector}
A vector can be described by its length \& direction. \\
Let $\vect{v} \in \mathbb{R}^2\backslash\{\vect{0}\}$. \\
Then $\vect{v} = ||\vect{v}||\vect{u}$ where $\vect{u}$ is the unit vector, $\vect{u} = \begin{pmatrix} cos\theta \\ sin\theta \end{pmatrix}$ \\
Thus $\forall\ \vect{v} \in \mathbb{R}^2\ \vect{v} = \begin{pmatrix} \lambda cos\theta \\ \lambda sin\theta \end{pmatrix}$ for some $\lambda \in \mathbb{R}$. \\

\subtitle{Definition 1.07 - }{Dot Product}
Let $\vect{v} \in \mathbb{R}^2$ and $\lambda \in \mathbb{R}$ such that $\vect{v} = (v_1,v_2)$. \\
Then $\vect{v} \dotprod \vect{w} = v_1.w_1 + v_2.w_2$. \\

\subtitle{Remark 1.08 - }{Positivity of Dot Product}
Let $\vect{v} \in \mathbb{R}^2$. \\
Then $\vect{v} \dotprod \vect{v} = ||\vect{v}||^2 = v_{1}^{2} + v_{2}^{2} \geq 0$. \\

\subtitle{Remark 1.09 - }{Angle between vectors in Euclidean Plane}
Let $\vect{v}, \vect{w} \in \mathbb{R}^2$. \\
Set $\theta$ to be the angle between $\vect{v}$ \& $\vect{w}$. \\
Then $$cos\theta = \frac{\vect{v} \dotprod \vect{w}}{||\vect{v}||\ ||\vect{w}||}$$.

\subtitle{Theorem 1.10 - }{Cauchy-Schwarz Inequality}
Let $\vect{v}, \vect{w} \in \mathbb{R}^2$. \\
Then $$|\vect{v} \dotprod \vect{w}| \leq ||\vect{v}||\ ||\vect{w}||$$
%
\textit{Proof}
\begin{align*}
  \frac{v_1w_1}{||\vect{v}||\ ||\vect{w}||} + \frac{v_2w_2}{||\vect{v}||\ ||\vect{w}||} &\leq \frac{1}{2}\left(\frac{v_{1}^{2}}{||\vect{v}||^2} + \frac{w_{1}^{2}}{||\vect{w}||^2}\right) + \frac{1}{2}\left(\frac{v_{2}^{2}}{||\vect{v}||^2} + \frac{w_{2}^{2}}{||\vect{w}||^2}\right) \\
  &\leq \frac{1}{2}\left(\frac{v_{1}^{2} + v_{2}^{2}}{||\vect{v}||^2} + \frac{w_{1}^{2} + w_{2}^{2}}{||\vect{w}||^2}\right) \\
  &\leq \frac{1}{2}(1 + 1) \\
  &\leq 1 \\
  => |v_1w_1 + v_2w_2| &\leq ||\vect{v}||\ ||\vect{w}|| \\
  |\vect{v} \dotprod \vect{w}| &\leq ||\vect{v}||\ ||\vect{w}|| \\
\end{align*}

\subsection{Complex Numbers}

\subtitle{Definition 1.11 - }{i}
\begin{alignat*}{1}
  i^2 &= -1 \\
  i &= \sqrt{-1}
\end{alignat*}

\subtitle{Definition 1.12 - }{Complex Number Set}
The set of complex numbers contains all numbers with an imaginary part. $$\mathbb{C} := \left\{x + iy; x,y \in \mathbb{R}\right\}$$
Complex numbers are often denoted by $$z = x + iy$$ and we say $x$ is the real part of $z$ and $y$ the imaginary part.

\subtitle{Definition 1.13 - }{Complex Conjugate}
Let $z \in \mathbb{C}$ st $z = x + iy$. \\
Then $$\bar{z} := x - iy$$
%
\newpage
%
\subtitle{Theorem 1.14 - }{Operations on Complex Numbers}
Let $z_1,z_2 \in \mathbb{C}$ st $z_1 = x_1 + iy_1$ and $z_2 = x_2 + iy_2$. \\
Then \begin{alignat*}{2}
  &z_1 + z_2 &&:= (x_1 + x_2) + i(y_1 + y_2) \\
  &z_1.z_2 &&:= (x_1 + iy_1)(x_2 + iy_2) \\
  & && := x_1.x_2 - y_1.y_2 + i(x_1.y_2 + x_2.y_1)
\end{alignat*}
\underline{N.B.} When dividing by a complex number, multiply top and bottom by the complex conjugate. \\

\subtitle{Definition 1.15 - }{Modulus of Complex Numbers}
The modulus of a complex number is the distance of the number, from the origin, on an Argand diagram.
Let $z \in \mathbb{C}$ st $z = x + iy$. \\
Then \begin{alignat*}{2}
  |z| &:= \sqrt{x^2 + y^2} \\
  &:= \sqrt{\bar{z}z}
\end{alignat*}
\underline{N.B.} Amplitude is an alternative name for the modulus \\

\subtitle{Definition 1.16 - }{Phase of Complex Numbers}
The phase of a complex number is the angle between the positive real axis and the line subtended from the origin and the number, on an Argand digram.$$z = |z|.(cos\theta + i.sin\theta), \quad \theta = \mathrm{Phase}$$
\underline{N.B.} Phase of $\bar{z}$ = - Phase of $z$ \\

\subtitle{Theorem 1.17 - }{de Moivre's Formula}
$$z^n = (cos\theta +i.sin\theta)^n = cos(n\theta)+i.sin(n\theta)$$

\subtitle{Theorem 1.18 - }{Euler's Formula}
$$e^{i\theta} = cos\theta + i.sin\theta$$

\subtitle{Remark 1.19}{}
Using Euler's formula we can express all complex numbers in terms of $e$. Thus many properties of the exponential remain true:
\begin{alignat*}{2}
  z &= \lambda e^{i\theta}, && \quad \lambda \in \mathbb{R} , \theta \in \left[0, 2\pi\right) \\
  => z_1 + z_2 &= \lambda_1 . \lambda_2. e^{i(\theta_1 + \theta_2)} \\
  \&, \frac{z_1}{z_2} &= \frac{\lambda_1}{\lambda_2}.e^{i(\theta_1 = \theta_2)}
\end{alignat*}

\newpage
\section{Euclidean Space, $\mathbb{R}^n$}
%
\subtitle{Definition 2.01 - }{Euclidean Space}
Let $n \in \mathbb{N}$ then $\forall\ \vect{x} = (x_1, x_2, ... , x_n)$ with $x_1, x_2, ... , x_n \in \mathbb{R}$ we have that $\vect{x} \in \mathbb{R}^n$. \\

\subtitle{Theorem 2.02 - }{Operations in Euclidean Space}
Let $\vect(x), \vect(y) \in \mathbb{R}^n$ and $\lambda \in \mathbb{R}$.
Then $$\vect(x) + \vect(y) = (x_1 + y_1, ... , x_n + y_n)$$
And $$\vect(x) + \lambda.\vect(y) = (x_1 + \lambda.y_1, ... , x_n + \lambda.y_n)$$

\subtitle{Definition 2.03 - }{Cartesian Product}
Let $A, B \in \mathbb{R}^n$ be non-empty sets. \\
Then $$A \times B := \{(a,b); a \in A, b \in B\}$$

\subsection{Dot Product}
%
\subtitle{Definition 2.04 - }{Dot Product}
Let $\vect{v}, \vect{w} \in \mathbb{R}^n$. Then \begin{alignat*}{1}
\vect{v} \dotprod \vect{w} &:= v_1.w_1 + ... + v_n.w_n \\
&:= \sum_{j=1}^n v_j.w_j
\end{alignat*}

\subtitle{Theorem 2.05 - }{Properties of the Dot Product}
Let $\vect{u}, \vect{v}, \vect{w} \in \mathbb{R}^n$.
Linearity: $$(\vect{u} + \lambda\vect{v}) \dotprod \vect{w} = \vect{u} \dotprod \vect{w} + \lambda(\vect{v} \dotprod \vect{w})$$
Symmetry: $$\vect{v} \dotprod \vect{w} = \vect{w} \dotprod \vect{v}$$
Positivity: $$\vect{v} \dotprod \vect{v} = v_1^2 + v_2^2 + ... +v_n^2 \geq 0$$

\subtitle{Definition 2.06 - }{Orthogonality}
Let $\vect{v}, \vect{w} \in \mathbb{R}^n$. \\
It is said that $\vect(v), \vect(w)$ are orthogonal to each other if $\vect{v} \dotprod \vect{w} = 0$ \\
\underline{N.B.} Orthogonal vectors are perpendicular to each other. \\

\subtitle{Definition 2.07 - }{The Norm}
Let $\vect{x} \in \mathbb{R}^n$.\\
Then $$||\vect{x}|| = \sqrt{\vect{x} \dotprod \vect{x}} = \sqrt{\sum_{i=1}^n x_i^2}$$

\newpage
%
\subtitle{Theorem 2.08 - }{Properties of the Norm}
Let $\vect{x}, \vect{y} \in \mathbb{R}^n$ and $\lambda \in \mathbb{R}$. Then
\begin{alignat*}{2}
  &||\vect{x}|| &&\geq 0 \\
  &||\vect{x}|| &&= 0 \mathrm{\ iff\ } \vect{x} = 0 \\
  &||\lambda\vect{x}|| &&= |\lambda| ||\vect{x}|| \\
  &||\vect{x} + \vect{y}|| &&\leq ||\vect{x}|| + ||\vect{y}||
\end{alignat*}

\subtitle{Theorem 2.09 - }{Dot Product and Norm}
Let $\vect{x}, \vect{y} \in \mathbb{R}^n$.
$$|\vect{x} \dotprod \vect{y}| \leq ||\vect{x}|| ||\vect{y}||$$
\underline{N.B.} $|\vect{x} \dotprod \vect{y}| = ||\vect{x}|| ||\vect{y}||$ iff $\vect{x}$ \& $\vect{x}$ are orthogonal.\\

\subtitle{Theorem 2.10 - }{Angle between Vectors}
Let $\vect{x}, \vect{y} \in \mathbb{R}^n$. Then $$cos\theta = \frac{\vect{x} \dotprod \vect{y}}{||\vect{x}|| ||\vect{y}||}$$

\subsection{Linear Subspaces}
%
\subtitle{Definition 2.11 - }{Linear Subspace}
Let $V \subset \mathbb{R}^n$. $V$ is a \textit{Linear Subspace} if: \\
\textbf{i)} $V \not = \emptyset$; \\
\textbf{ii)} $\forall\ \vect{v}, \vect{w} \in V$ then $\vect{v} + \vect{w} \in V$; \\
\textbf{iii)} $\forall\ \lambda \in \mathbb{R}, \vect{v} \in V$ then $\lambda\vect{v} \in V$. \\

\subtitle{Definition 2.12 - }{Span}
Let $\vect{x_1}, ... , \vect{x_k} \in \mathbb{R}^n; k \in \mathbb{N}$. Then
$$\mathrm{span}\{\vect{x_1}, ... , \vect{x_k}\} := \{\lambda_1\vect{x_1} + ... + \lambda_k\vect{x_k}; \lambda_i \in \mathbb{R}, 0 \leq i \geq k\}$$

\subtitle{Definition 2.13 - }{Spans are Subspaces}
Let $\vect{x_1}, ... , \vect{x_k} \in \mathbb{R}^n;\ k \in \mathbb{N}$. Then span$\{\vect{x_1}, ... , \vect{x_k}\}$ is a linear subspace of $\mathbb{R}^n$. \\

\subtitle{Theorem 2.14}{}
$$W_{\vect{a}} := \{\vect{x} \in \mathbb{R}^n; \vect{x} \dotprod \vect{a} = 0\}\mathrm{\ is\ a\ subspace.}$$

\subtitle{Definition 2.15 - }{Orthogonal Complement}
Let $V \subset \mathbb{R}^n$. Then,
$$V^{\perp} := \{\vect{x} \in \mathbb{R}^n; \vect{x} \dotprod \vect{y}\ \forall\ \vect{y} \in V\}$$
\underline{N.B.} $V^{\perp} \subset \mathbb{R}^n$

\subtitle{Theorem 2.16 - }{Relationship of Subspaces}
Let $V, W$ be subspaces of $\mathbb{R}$. Then
$$V \cap W \mathrm{\ is\ a\ subspace.}$$
$$V+W := \{\vect{v} + \vect{w}; \vect{v} \in V, \vect{w} \in W\}\mathrm{\ is\ a\ subspace.}$$

\subtitle{Definition 2.17 - }{Direct Sum}
Let $V_1, V_2, W$ be subspaces of $\mathbb{R}$. Then $W$ is said to be a \textit{direct sum} if \\
\textbf{i)} $W = V_1 + V_2$; \\
\textbf{ii)} $V_1 \cap V_2 = \emptyset$. \\

\section{Linear Equations \& Matrices}

\subsection{Linear Equations}
%
\subtitle{Definition 3.01 - }{Multi-Variable Linear Equations}
Linear equations produce a straight line and can have multiple variables. \\
\textit{Examples} - $x = 3, y = x + 3, z + 5x - 2y$ \\

\subtitle{Defintion 3.02 - }{Systems of Linear Equations}
Let $\vect{a}, \vect{x} \in \mathbb{R}^n$ \& $b \in \mathbb{R}$ such that $\vect{a} \dotprod \vect{x} = b$. \\
$\vect{a} \dotprod \vect{x} = b$ is a linear equation in $x$ with $S = \{\vect{x}; \vect{a} \dotprod \vect{x} = b\}$ as the set of solutions. \\
\underline{N.B.} If $b = 0$ then $S(\vect{a}, 0)$ is a subspace. \\

\subsection{Matrices}
%
\subtitle{Definition 3.03 - }{Matrix}
Let $m, n \in \mathbb{N}$, then a $m \times n$ grid of numbers form an "m" by "n" matrix.
Each element of the matrix can be reference by $a_{ij}$ with $i = 1, ... , m$ and $j = 1, ... , n$.
$$ A = \begin{pmatrix}
  a_{11} & a_{12} & ... & a_{1n} \\
  a_{21} & a_{22} & ... & a_{2n} \\
  \vdots & \vdots & \ddots & \vdots \\
  a_{m1} & a_{m2} & ... & a_{mn}
\end{pmatrix}
$$
\underline{N.B.} m,i = rows, n,j = columns \\

\subtitle{Definition 3.04 - }{Sets of Matrices}
$M_{m,n}(\mathbb{R})$ is the set of m x n matrices containing only real numbers. \\
$M_{m,n}(\mathbb{Z})$ is the set of m x n matrices containing only integers. \\
$M_{n}(\mathbb{R})$ is the set square matrices, size n, containing only real numbers. \\

\subtitle{Definition 3.05 - }{Transpose Vectors}
Let $\vect{x} = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n\end{pmatrix}$ then $\vect{x}^t = \begin{pmatrix} x_1 & x_2 & ... & x_n\end{pmatrix}$

\subtitle{Definition 3.06 - }{Vector-Matrix Multiplication}
Let $A \in \mathbb{R}_{m,n}$ and $\vect{x} \in \mathbb{R}^n$ then
$$A\vect{x} := \begin{pmatrix}
  a_{11} & a_{12} & ... & a_{1n} \\
  a_{21} & a_{22} & ... & a_{2n} \\
  \vdots & \vdots & \ddots & \vdots \\
  a_{m1} & a_{m2} & ... & a_{mn}
\end{pmatrix} \begin{pmatrix}
  x_1 \\ x_2 \\ \vdots \\ x_n
\end{pmatrix} = \begin{pmatrix}
  a_{11}x_1 + a_{12}x_2 + ... + a_{1n}{x_n} \\
  a_{21}x_1 + a_{22}x_2 + ... + a_{2n}{x_n} \\
  \vdots \\
  a_{m1}x_1 + a_{m2}x_2 + ... + a_{mn}{x_n} \\
\end{pmatrix} = \begin{pmatrix}
  \vect{a}_{1}^{t} \dotprod \vect{x} \\
  \vect{a}_{2}^{t} \dotprod \vect{x} \\
  \vdots \\
  \vect{a}_{m}^{t} \dotprod \vect{x} \\
\end{pmatrix} \in \mathbb{R}^m
$$ This can be simplified to $$\vect{y} = A\vect{x} \mathrm{\ with\ } y_i = \sum_{j=1}^n a_{ij} x_j$$

\subtitle{Theorem 3.07 - }{Operations on Matrices with Vectors}
\begin{enumerate}[label=\roman*)]
  \item $A(\vect{x} + \vect{y}) = A\vect{x} + A\vect{y},\quad \forall\ \vect{x}, \vect{y} \in \mathbb{R}^n$.
  \item $A(\lambda\vect{x}) = \lambda(A\vect{x}),\quad \forall \vect{x} \in \mathbb{R}^n, \lambda \in \mathbb{R}$.
\end{enumerate}

\subtitle{Theorem 3.08}{}
Let $A = (a_{ij}) \in M_{m,n}(\mathbb{R})$ and $B = (b_{ij}) \in M_{l,m}(\mathbb{R})$.
Then there exists a $C = (c_{ij}) \in M_{l,n}(\mathbb{R})$ such that
$$C\vect{x} = B(A\vect{x}), \quad \forall\ \vect{x} \in \mathbb{R}^n$$
\underline{N.B.} $c_{ij} = \sum_{k=1}^m b_{ik} a_{kj}$ \\

\subtitle{Theorem 3.09 - }{Operation between Matrices}
Let $A, B \in M_{m,n}$ and $C \in M_{l,m}$
\begin{enumerate}[label=\roman*)]
  \item $C(A + B) = CA + CB$.
  \item $(A + B)C = AC + BC$.
  \item Let $D \in M_{m,n}, E \in M_{n,l}$ \& $F \in M_{l, k}$ then $$E(FG) = (EF)G$$
\end{enumerate}
\underline{N.B.} $AB \not = BA$ \\

\subtitle{Definition 3.10 - }{Types of Matrix}
Upper Triangle $\begin{pmatrix}
1 & 2 & 3 \\
0 & 4 & 5 \\
0 & 0 & 6
\end{pmatrix},\quad a_{ij} = 0 \mathrm{\ if\ } i > j$. \\
Lower Triangle $\begin{pmatrix}
1 & 0 & 0 \\
2 & 3 & 0 \\
4 & 5 & 6
\end{pmatrix},\quad a_{ij} = 0 \mathrm{\ if\ } i < j$. \\
Symmetric Matrix $\begin{pmatrix}
1 & 2 & 3 \\
2 & 4 & 0 \\
3 & 0 & 1
\end{pmatrix},\quad a_{ij} = a_{ji}.$ \\
Anti-Symmetric $\begin{pmatrix}
1 & -2 & -3 \\
2 & 0 & -4 \\
3 & 4 & -1
\end{pmatrix},\quad a_{ij} = -a_{ji}$. \\

\subtitle{Definition 3.11 - }{Transposed Matrices}
Let $A = (a_{ij}) \in M_{m,n}(\mathbb{R})$ then the transponse of $A$, $A^t$, is an element of $M_{n,m}(\mathbb{R})$. $$A^t := (aji)$$

\subtitle{Theorem 3.12 - }{Transpose Matrix Multiplication}
Let $A \in M_{m,n}(\mathbb{R}), \vect{x} \in \mathbb{R}^n, \vect{y} \in \mathbb{R}^m$. Then
$$\vect{y} \dotprod A \vect{x} = \left(A_t \vect{y}\right) \dotprod \vect{x}$$

\subtitle{Theorem 3.10 - }{Transposing Multiplied Matrices}
$$(AB)^t = B^t A^t$$

\subsection{Structure of Set of Solutions}
%
\subtitle{Definition 3.13 - }{Set of Solutions}
Let $A \in M_{m,n}(\mathbb{R})$ and $\vect{b} \in \mathbb{R}^m$. Then $$S(A, \vect{b}) :={\vect{x} \in \mathbb{R}^n; A\vect{x} = b}$$

\subtitle{Definition 3.14 - }{Homogenous Solutions}
The system of $S(A, \vect{0})$ is called said to be \textit{homogenous}. All other systems are \textit{inhomogenous}.
\underline{N.B.} - $S(A, \vect{0})$ is a linear subspace. \\

\subtitle{Theorem 3.15 - }{Using Homogenous Solutions}
Let $A \in M_{m,n}(\mathbb{R})$ and $\vect{b} \in \mathbb{R}^n$. Let $\vect{x}_0 \in \mathbb{R}^n$ such that $A\vect{x}_0 = \vect{b}$, then
$$S(A, \vect{b}) = {\vect{x}_0} + S(A, \vect{0})$$

\subtitle{Remark 3.16 - }{Systems of Linear Equations as Matrices}
The system of linear equations $3x + z = 0, y - z = 1, 3x + y = 1$ can be represented by a matrix and a vector.
$$A = \begin{pmatrix}
3 & 0 & 1 \\
0 & 1 & -1 \\
3 & 1 & 0
\end{pmatrix},\ \vect{b} = \begin{pmatrix}
0 \\ 1 \\ 1
\end{pmatrix}$$

\subsection{Solving Systems of Linear Equations}
Systems of linear equations can be displayed as matrices which can be reduced and solved by a technique called \textit{Gaussian Elimination}. \\ \\
%
\subtitle{Theorem 3.17}{}
There are certain operations that can be performed on a system of linear equations without changing the result:
\begin{enumerate}[label=\roman*)]
  \item Multiply an equaion by a non-zero constant;
  \item Add a multiple of any equation to another equation;
  \item Swap any two equations.
\end{enumerate}

\subtitle{Definition 3.18 - }{Augmented Matrices}
Let $A\vect{x} = \vect{b}$ be a system of linear equations. The associated \textit{Augmented Matrix} is $$(A\ \vect{b}) \in M_{m, n+1}(\mathbb{R})$$

\subtitle{Theorem 3.19 - }{Elementary Row Operations}
From \textit{Theorem 3.17} we can deduce ceratin operations that can be performed on an \textit{Augmented Matrix} which do not alter the solutions:
\begin{enumerate}[label=\roman*)]
  \item Multiply a row by a non-zero constant, $row\ i \to \lambda(row\ i)$;
  \item Add a multiple of any row to another row, $row\ i \to row\ i + \lambda(row\ j)$;
  \item Swap two rows, $row\ i \leftrightarrow row\ j $.
\end{enumerate}

\subtitle{Definition 3.20 - }{Row Echelon Form}
A matrix is in \textit{Row Echelon Form} if:
\begin{enumerate}[label=\roman*)]
  \item The left-most non-zero value in each row is $1$; And,
  \item The leading $1$ in each row is one place to the right of the leading $1$ in the row below.
\end{enumerate}
\textit{Example} $$\begin{pmatrix}
  1 & a & b \\
  0 & 1 & c \\
  0 & 0 & 1
\end{pmatrix}$$

\subtitle{Definition 3.20 - }{Reduced Row Echelon Form}
A matrix is in \textit{Reduced Row Echelon Form} if:
\begin{enumerate}[label=\roman*)]
  \item The matrix is in \textit{row echelon form}; And,
  \item All values in a row, except the leading $1$, are $0$.
\end{enumerate}
\textit{Example} $$\begin{pmatrix}
  1 & 0 & 0 \\
  0 & 1 & 0 \\
  0 & 0 & 1
\end{pmatrix}$$

\subtitle{Theorem 3.21 - }{Gaussian Elimination}
\textit{Gaussian Elimination} is a technique used to solve systems of linear equations.
\textit{Example} \\
Solve $x + y + 2z = 9, 2x + 4y - 3z = 1, 3x + 6y - 5z = 0$.
\begin{alignat*}{2}
  \mathrm{Augmented\ Matrix\ -\ } &\begin{pmatrix}
    1 & 1 & 2 & 9 \\
    2 & 4 & -3 & 1 \\
    3 & 6 & -5 & 0
  \end{pmatrix} && \\
  \mathrm{By\ EROS\ -\ } &\begin{pmatrix}
    1 & 1 & 2 & 9 \\
    2 & 4 & -3 & 1 \\
    3 & 6 & -5 & 0
  \end{pmatrix} &&= \begin{pmatrix}
    1 & 1 & 2 & 9 \\
    0 & 2 & -7 & -17 \\
    0 & 3 & -11 & 27
  \end{pmatrix} \\ &  &&= \begin{pmatrix}
    1 & 1 & 2 & 9 \\
    0 & 2 & -7 & -17 \\
    0 & 1 & -4 & -10
  \end{pmatrix} \\ & &&= \begin{pmatrix}
    1 & 1 &2 & 9 \\
    0 & 1 & -4 & -10 \\
    0 & 2 & -7 & -17
  \end{pmatrix} \\ & &&= \begin{pmatrix}
    1 & 1 & 2 & 9 \\
    0 & 1 & -4 & -10 \\
    0 & 0 & 1 & 3
  \end{pmatrix} \\ & &&= \begin{pmatrix}
    1 & 0 & 6 & 19 \\
    0 & 1 & -4 & -10 \\
    0 & 0 & 1 & 3
  \end{pmatrix}\\ & &&= \begin{pmatrix}
    1 & 0 & 0 & 1 \\
    0 & 1 & 0 & 2 \\
    0 & 0 & 1 & 3
  \end{pmatrix}\\
  => & \underline{x = 1, y= 2, z = 3}
\end{alignat*}

\subsection{Elementary Matrices \& Inverting Matrices}
%
\subtitle{Definition 3.22 - }{Invertible Matrices}
A matrix, $A \in M_{m,n}(\mathbb{R})$, is said to be \textit{Invertible} if there exists $A^{-1} \in M_{n,m}(\mathbb{R})$ such that
$$AA^{-1} = I$$
\underline{N.B.} - If a matrix is not invertible then it is \textit{Singular}.\\

\subtitle{Definition 3.23 - }{Elementary Matrices}
A matrix, $E \in M_{m,n}(\mathbb{R})$, is said to be an \textit{Elementary Matrix} if it can be obtained by performing Elementary Row Operations on a square identity matrix.\\
\textit{Examples}
$\begin{pmatrix}
  0 & 1 \\
  1 & 0
\end{pmatrix},\ \begin{pmatrix}
  0 & \lambda \\
  \mu & 0
\end{pmatrix}$ \\

\subtitle{Remark 3.24}{}
All elementary matrices are invertible.

\subtitle{Remark 3.25}{}
Let $A$ be a matrix, and $B$ be a matrix which can be obtained from $A$ by elementary row operations. Then there exists an elementary matrix $E$ such that $$B = EA$$.

\subtitle{Theorem 3.26 - }{Finding $A^{-1}$}
Let $A = \begin{pmatrix}
  a_{11} & a_{12} & a_{13} \\
  a_{21} & a_{22} & a_{23} \\
  a_{31} & a_{32} & a_{33}
\end{pmatrix},\ B = \begin{pmatrix}
  b_{11} & b_{12} & b_{13} \\
  b_{21} & b_{22} & b_{23} \\
  b_{31} & b_{32} & b_{33}
\end{pmatrix},\ I = \begin{pmatrix}
  1 & 0 & 0 \\
  0 & 1 & 0 \\
  0 & 0 & 1
\end{pmatrix}$. \\
Then by using EROS to change $(A\ I) \to (I\ B)$, $B$ is the inverse of $A$. \\

\subtitle{Theorem 3.27 - }{Inverse of a 2x2 Matrix}
Let $A = \begin{pmatrix}
  a & b \\
  c & d
\end{pmatrix}$ then $$A^{-1} = \frac{1}{ad - bc}\begin{pmatrix}
  d & -b \\
  -c & a
\end{pmatrix}$$

\end{document}
